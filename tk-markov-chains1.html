<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Stochastic Processes & Finance – Summer Reading Project</title>
  <link href="https://fonts.googleapis.com/css2?family=Lato&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Lato', sans-serif;
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      color: #333;
    }
    header {
      background-color: #003366;
      color: white;
      padding: 1em;
      text-align: center;
    }
    nav {
      background-color: #e0e0e0;
      padding: 1em;
      text-align: center;
    }
    nav a {
      margin: 0 1em;
      text-decoration: none;
      color: #003366;
      font-weight: bold;
    }
    section {
      max-width: 1200px;
      margin: 2em auto;
      padding: 1em;
      background: white;
      border-radius: 8px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }

    .nav-bottom a {
      flex: 1;
      text-align: center;
      padding: 0.5em 1em;
      color: #003366;
      font-weight: bold;
      text-decoration: none;
    }

    footer {
      text-align: center;
      font-size: 0.9em;
      color: #777;
      margin: 2em 0;
    }
    .hint-text {
      display: none;
      margin-top: 0.5em;
      background: #f0f0f0;
      padding: 0.5em;
      border-left: 4px solid #0077cc;
      border-radius: 4px;
    }

    .hint-button {
      margin-top: 1em;
      background-color: #0077cc;
      color: white;
      border: none;
      padding: 0.4em 0.8em;
      border-radius: 4px;
      cursor: pointer;
      font-size: 0.9em;
    }

    .hint-button:hover {
      background-color: #005fa3;
    }
  </style>
</head>
<body>

<header>
  <h1>Chapter 2: Markov Chains: Introduction</h1>
  <p>Study notes and chapter summaries</p>
</header>

<nav>
  <a href="index.html">Home</a>
  <a href="topics.html">Topics</a>
  <a href="notes.html">Notes</a>
  <a href="projects.html">Programming</a>
</nav>
<section>
<h2>1. Definition</h2>
<p>
  A Markov process \(X_t\) is a stochastic process with the property that, given the value of \(X_t\), the values of \(X_s\) for s>t are not influenced by the values of \(X_u\)  for \(t> u\).  
</p>
<p>
  A <strong>discrete-time Markov chain </strong> is a Markov process whose state space is a finite or countable set, and whose (time) index set is T =\( (0,1,2 \ldots ) \). Mathematically:
  $$ P(X_{n+1} = j| X_0 = i_0, \ldots , X_{n-1} = i_{n-1}, X_n= i) $$
  $$ = P( X_{n+1} =j | X_n= i) $$
  for all n and all states \( i_0 , \ldots, i_{n-1}, i, j \)
</p>
<h3>Understanding One-Step Transition Probabilities</h3>
<p>
In a Markov chain, we usually label the state space using non-negative integers: 
<strong>0, 1, 2, ...</strong>. Unless stated otherwise, we follow this convention.
</p>
<p>
If the chain is in a certain state at time \( n \), we say \( X_n = i \).
The probability that it moves to state \( j \) at the next time step \( n+1 \),
given that it is currently in state \( i \), is called the 
<strong>one-step transition probability</strong>. It is denoted by:
</p>
\[
P^{(n,n+1)}_{ij} = \Pr(X_{n+1} = j \mid X_n = i)
\]
<p>
This notation indicates that the transition probabilities can depend on the current time \( n \),
as well as the current and next states \( i \) and \( j \).
</p>
<p>
If these probabilities are independent of time, we say that the Markov chain has 
<strong>stationary transition probabilities</strong>. In that case, we write:
</p>
\[
P_{ij} = \Pr(X_{n+1} = j \mid X_n = i)
\]
<p>
This means the probability of transitioning from state \( i \) to state \( j \)
remains the same at every step.
</p>
<p>
These probabilities \( P_{ij} \) are usually arranged in a 
<strong>transition matrix</strong> \( P \) as follows:
</p>
\[
P = 
\begin{bmatrix}
P_{00} & P_{01} & P_{02} & \cdots \\
P_{10} & P_{11} & P_{12} & \cdots \\
P_{20} & P_{21} & P_{22} & \cdots \\
\vdots & \vdots & \vdots & \ddots \\
\end{bmatrix}
\]
<p>
Each row corresponds to the current state \( i \), and each column represents the
probability of transitioning to state \( j \).
</p>
<h3>The Markov Transition Matrix</h3>

<p>
In a Markov chain, we represent the one-step transition probabilities using a matrix \( P \),
called the <strong>Markov matrix</strong> or <strong>transition probability matrix</strong>.
Each entry \( P_{ij} \) in this matrix represents the probability of moving from state \( i \)
to state \( j \) in one step.
</p>

<p>
That is,
\[
P_{ij} = \Pr(X_{n+1} = j \mid X_n = i)
\]
</p>

<p>
The \( i \)th row of \( P \) gives the full probability distribution of the next state \( X_{n+1} \)
given that the current state \( X_n = i \). In other words, the \( i \)th row shows where the process
might go from state \( i \), and with what probabilities.
</p>

<p>
If the number of states is finite (say, \( 0, 1, \ldots, r \)), then \( P \) is a finite square matrix 
of order \( r + 1 \), meaning it has \( r+1 \) rows and \( r+1 \) columns.
</p>

<h4>Conditions for Valid Transition Matrices</h4>

<p>
The entries of the transition matrix must satisfy two conditions:
</p>

<ul>
  <li>
    <strong>Non-negativity:</strong> All entries must be non-negative: 
    \[
    P_{ij} \geq 0 \quad \text{for all } i, j
    \]
  </li>
  <li>
    <strong>Row sums equal to 1:</strong> The probabilities in each row must sum to 1:
    \[
    \sum_{j=0}^{r} P_{ij} = 1 \quad \text{for all } i
    \]
  </li>
</ul>

<p>
The second condition simply reflects the fact that <em>some</em> transition must occur at each step. 
Even if the state doesn't change (i.e., it stays in the same state), we still consider that a 
transition has occurred.
</p>
<h3>Theorem: A Markov process is completely defined once we specify its Markov matrix P and its initial state \( X_0 \)</h3>
<p> Let P{ \( X_0 = i \)} = \(p_{i_0}\). It's enough to show how to compute the probability P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_n= i_n \) }, since any probability containing \(X_{j_1}, X_{j_2}, \ldots, X_{j_k} , j_k> \ldots >j_2 >j_1  \) can be obtained by summing terms of the above form (by Theorem of Total Probability).
<p>P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_n= i_n \) } </p> 
<p>=P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_{n-1}= i_{n-1} \) } P{ \(X_n = i_n | X_0 = i_0, X_1 =i_1, \ldots , X_{n-1}= i_{n-1} \) }</p>
<p>=P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_{n-1}= i_{n-1} \) } P{ \(X_n = i_n | X_{n-1}=i_{n-1} \) } </p>
<p>=P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_{n-1}= i_{n-1} \) } \(P_{i_{n-1}i_n}\)</p>
<p>Similarly, P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_n= i_{n-1} \) }</p>
<p>=P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_{n-2}= i_{n-2} \) } \(P_{i_{n-2}i_{n-1}}\)</p>
<p>Therefore, the original expression simplifies to:</p>
<p>
  \( P\left( X_0 = i_0, X_1 = i_1, \ldots , X_n = i_n \right) = p_{i_0} \cdot P_{i_0 i_1} \cdot P_{i_1 i_2} \cdot \ldots \cdot P_{i_{n-1} i_n} \)
</p>
<h2>2 Transition Probability Matrices of a Markov Chain: Understanding Multi-Step Transition Probabilities</h2>

<p>
A Markov chain is fully defined by two things:
<ul>
  <li>Its one-step transition probability matrix \( P \)</li>
  <li>Its initial state distribution (i.e., the probabilities of starting in each state)</li>
</ul>
</p>

<p>
Most of the analysis of Markov chains revolves around calculating the probabilities of different outcomes after several steps or time periods. 
For this, we use the <strong>n-step transition probability matrix</strong>, denoted by \( P^{(n)} \).
</p>

<p>
The element \( P^{(n)}_{ij} \) represents the probability that the process moves from state \( i \) to state \( j \) in exactly \( n \) steps. Formally:
</p>

<p style="text-align: center;">
  \( P^{(n)}_{ij} = \Pr(X_{m+n} = j \mid X_m = i) \)
</p>

<p>
Since we are considering <em>stationary</em> (time-homogeneous) Markov chains, the probability depends only on the number of steps \( n \), and not on the starting time \( m \).
</p>

<p>
Thanks to the Markov property (which says that the future depends only on the present state, not the past), we can compute \( P^{(n)} \) recursively:
</p>

<p style="text-align: center;">
  \( P^{(n)}_{ij} = \sum_{k=0}^{\infty} P_{ik} \cdot P^{(n-1)}_{kj} \)
</p>

<p>
This recursive formula is just standard matrix multiplication, so we can write:
</p>

<p style="text-align: center;">
  \( P^{(n)} = P \cdot P^{(n-1)} \)
</p>

<p>
By repeatedly applying this formula (i.e., multiplying the matrix \( P \) with itself \( n \) times), we get:
</p>

<p style="text-align: center;">
  \( P^{(n)} = P^n \)
</p>

<p>
That is, the n-step transition probabilities are exactly the entries of the matrix \( P \) raised to the \( n \)-th power.
</p>

<p>
To complete the definition, we also define:
</p>

<ul>
  <li>\( P^{(0)}_{ij} = 1 \) if \( i = j \)</li>
  <li>\( P^{(0)}_{ij} = 0 \) if \( i \neq j \)</li>
</ul>

<p>
This just means that in zero steps, the process stays where it is.
</p>
<h3>Proof of the n-Step Transition Probability Formula</h3>

<p>
We want to prove the formula:
</p>

<p style="text-align: center;">
  \( P^{(n)}_{ij} = \sum_{k} P_{ik} \cdot P^{(n-1)}_{kj} \)
</p>

<p>
This formula means: the probability of going from state \( i \) to state \( j \) in \( n \) steps is the sum over all intermediate states \( k \) of the probability of:
</p>
<ol>
  <li>Going from \( i \) to \( k \) in one step: \( P_{ik} \), and</li>
  <li>Then from \( k \) to \( j \) in \( n-1 \) steps: \( P^{(n-1)}_{kj} \)</li>
</ol>

<p>
To see why this works, let’s look at what it means to reach \( j \) from \( i \) in exactly \( n \) steps.
</p>

<p>
We imagine that the first step takes the process from state \( i \) to some state \( k \). Then the remaining \( n - 1 \) steps must take the process from \( k \) to \( j \).
</p>

<p>
Now, because of the <strong>Markov property</strong>, once the process reaches \( k \), it "forgets" how it got there. So the probability of continuing from \( k \) to \( j \) depends only on \( k \), and not on any earlier states.
</p>

<p>
So, for each possible intermediate state \( k \), the probability of going from \( i \rightarrow k \rightarrow j \) in total \( n \) steps is:
</p>

<p style="text-align: center;">
  \( \Pr(X_1 = k \mid X_0 = i) \cdot \Pr(X_n = j \mid X_1 = k) = P_{ik} \cdot P^{(n-1)}_{kj} \)
</p>

<p>
To find the total probability of reaching \( j \) from \( i \) in \( n \) steps, we sum this product over all possible intermediate states \( k \):
</p>

<p style="text-align: center;">
  \( P^{(n)}_{ij} = \sum_{k} P_{ik} \cdot P^{(n-1)}_{kj} \)
</p>

<p>
This is exactly how matrix multiplication works, so we can write the matrix formula:
</p>

<p style="text-align: center;">
  \( P^{(n)} = P \cdot P^{(n-1)} \)
</p>

<p>
By repeating this step, we get:
</p>

<p style="text-align: center;">
  \( P^{(n)} = P^n \)
</p>

<p>
This completes the proof. It uses the basic idea that the path from \( i \) to \( j \) in \( n \) steps must go through some state \( k \) after the first step, and the rest follows by the Markov property and total probability.
</p>
<h2>3(a). Inventory Model Using Markov Chains</h2>

<p>
  This model describes a scenario where a commodity is stocked to meet a random demand over time. Time is divided into periods (n = 0, 1, 2, ...), and stock is reviewed at the end of each period.
</p>

<p>
  The demand in each period, denoted by \( \xi_n \), is a random variable with the same probability distribution across time:
</p>

<ul>
  <li>\( \Pr(\xi_n = 0) = a_0 \)</li>
  <li>\( \Pr(\xi_n = 1) = a_1 \)</li>
  <li>\( \Pr(\xi_n = 2) = a_2 \)</li>
  <li>... and so on, where \( \sum a_k = 1 \)</li>
</ul>

<p>
  The stock is restocked based on the following policy:
  <ul>
    <li>If the current stock is \( \leq s \), restock up to \( S \)</li>
    <li>If the stock is more than \( s \), no replenishment occurs</li>
  </ul>
</p>

<p>
  The state of the process is \( X_n \), the stock level at the end of period \( n \), before restocking. It evolves as a Markov chain based on the random demand and the restocking rule.
</p>
<p>
  The transition rule is:
</p>
<ul>
<li>If \( X_n > s \), then \( X_{n+1} = X_n - \xi_{n+1} \)</li>
<li>If \( X_n \leq s \), then \( X_{n+1} = S - \xi_{n+1} \)</li>
</ul>
<p>
  This rule, together with the demand probabilities, determines the transition probability matrix.
</p>
<h3>Example</h3>

  <p>
    Suppose:
  </p>
  <ul>
    <li>\( S = 2 \), \( s = 0 \)</li>
    <li>\( \Pr(\xi = 0) = 0.5 \), \( \Pr(\xi = 1) = 0.4 \), \( \Pr(\xi = 2) = 0.1 \)</li>
  </ul>

  <p>
    Possible stock levels: 2, 1, 0, -1 (unfilled demand is treated as negative stock).
  </p>

  <p>
    From this setup, we compute the transition probabilities. For instance:
    <ul>
      <li>If \( X_n = 1 \), no restocking. To reach \( X_{n+1} = 0 \), \( \xi_{n+1} = 1 \). So \( P_{10} = 0.4 \)</li>
      <li>If \( X_n = 0 \), restock to 2. To reach \( X_{n+1} = 0 \), \( \xi_{n+1} = 2 \). So \( P_{00} = 0.1 \)</li>
    </ul>
  </p>

  <p>
    Transition Matrix:
  </p>

  <pre>
       |  -1   0    1    2
    ------------------------
    -1 |  0   0.1  0.4  0.5
     0 |  0   0.1  0.4  0.5
     1 | 0.1  0.4  0.5   0
     2 |  0   0.1  0.4  0.5
  </pre>

  <h3>Important Quantities</h3>

  <ul>
    <li>Long-term probability of unfilled demand: \( \lim_{n \to \infty} \Pr(X_n < 0) \)</li>
    <li>Average inventory level: \( \lim_{n \to \infty} \sum_{j>0} j \cdot \Pr(X_n = j) \)</li>
  </ul>

  <h3>Visual Representation</h3>
  <p>Below is the illustration of how the process evolves over time:</p>
  <img src="tk-mc1img.png" alt="Inventory Process Diagram (Figure 3.1)">
<h2>3(b).Ehrenfest Urn Model</h2>
  <p>
    The Ehrenfest urn model is a classical representation of diffusion, simulating how molecules move across a membrane.
  </p>

  <p>
    Imagine you have two containers (urns), A and B, that together contain a total of <b>2a</b> balls. Initially, urn A holds <b>k</b> balls and urn B has <b>2a - k</b> balls.
  </p>

  <p>
    At each time step, one of the <b>2a</b> balls is selected at random, and then moved to the other urn. This random movement simulates the diffusion of molecules.
  </p>

  <p>
    Let <b>Y<sub>n</sub></b> be the number of balls in urn A after the <b>n<sup>th</sup></b> step, and define a centered variable:
  </p>
  <p><b>X<sub>n</sub> = Y<sub>n</sub> - a</b></p>

  <p>
    Then <b>X<sub>n</sub></b> becomes a Markov chain with state space:
  </p>
  <p><b>{ -a, -a+1, ..., 0, ..., a-1, a }</b></p>

  <h3>Transition Probabilities</h3>
  <p>
    The transition probabilities are defined as follows:
  </p>
  <ul>
    <li>
      \( P_{i,i+1} = \frac{a - i}{2a} \) &nbsp;&nbsp;&nbsp; (gain a ball in urn A)
    </li>
    <li>
      \( P_{i,i-1} = \frac{a + i}{2a} \) &nbsp;&nbsp;&nbsp; (lose a ball from urn A)
    </li>
    <li>
      All other transitions have zero probability.
    </li>
  </ul>

  <p>
    This transition rule reflects that if urn A has more balls (i is positive), then it's more likely to lose a ball, and vice versa. Thus, there is a tendency or "drift" toward balancing the urns.
  </p>

  <h3>Equilibrium Distribution</h3>
  <p>
    An important focus of this model is finding the <b>equilibrium distribution</b> — the long-run behavior of the number of balls in urn A. Over time, the system stabilizes such that the probability distribution over states no longer changes.
  </p>

  <p>
    The model is symmetric and reversible, so the equilibrium distribution exists and reflects the binomial distribution of balls across urn A and B, due to random uniform selection.
  </p>
<h2>3(c). Markov Chains in Genetics</h2>

  <h3>1. Simple Haploid Model (No Mutation or Selection)</h3>
  <p>
    This model investigates how gene frequencies fluctuate across generations. We assume:
    </p>
    <ul>
      <li>There are <b>2N haploid genes</b> in each generation (e.g., from 2N bacteria).</li>
      <li>Each gene is either of type <b>a</b> or <b>A</b>.</li>
      <li>If there are <b>j</b> a-genes in the current generation, then the probability of picking an a-gene to pass on is:
        <br><b>p<sub>j</sub> = j / (2N)</b>, and <b>q<sub>j</sub> = 1 − p<sub>j</sub></b>.
      </li>
      <li>The next generation is formed by independently selecting <b>2N genes</b> with replacement, using the above probabilities.</li>
    </ul>
  <p>
    So, the number of a-genes in the next generation follows a <b>Binomial(2N, p<sub>j</sub>)</b> distribution.
  </p>
  <p>
    We define a Markov chain <b>X<sub>n</sub></b> to be the number of a-genes in generation n. The state space is {0, 1, ..., 2N}.
  </p>

  <h4>Transition Probability:</h4>
  <p>
    The transition probability from state j to state k is:
  </p>
  <p>
    <b>P<sub>jk</sub> = C(2N, k) * (p<sub>j</sub>)<sup>k</sup> * (q<sub>j</sub>)<sup>2N−k</sup></b><br>
    where C(2N, k) = 2N choose k.
  </p>

  <h4>Fixation States:</h4>
  <ul>
    <li>State 0 (all A) and state 2N (all a) are <b>absorbing</b>.</li>
    <li>Once the population hits either of these, it stays there forever (no variation left).</li>
  </ul>

  <h3>2. Adding Mutation</h3>
  <p>
    Now suppose each gene may mutate before reproduction:
    </p>
    <ul>
      <li>a → A with probability <b>μ</b></li>
      <li>A → a with probability <b>ν</b></li>
    </ul>

  <h4>Modified Probability of a-gene after Mutation:</h4>
  <p>
    Let j be the number of a-genes initially. After mutation, the expected number of a-genes becomes:
    </p>
    <ul>
      <li>Each of the j a-genes mutates to A with probability μ → contributes <b>j(1 − μ)</b> a-genes.</li>
      <li>Each of the (2N − j) A-genes mutates to a with probability ν → contributes <b>(2N − j)ν</b> a-genes.</li>
    </ul>
  <p>
    Total expected number of a-genes after mutation = <b>j(1 − μ) + (2N − j)ν</b>
  </p>
  <p>
    So, the probability that a randomly selected gene after mutation is an a-gene is:
    <br><b>p<sub>j</sub> = [j(1 − μ) + (2N − j)ν] / (2N)</b>
  </p>
  <p>
    And <b>q<sub>j</sub> = 1 − p<sub>j</sub></b>
  </p>

  <h4>Consequences:</h4>
  <ul>
    <li>Now, even if all genes are of one type, mutation allows the other type to reappear.</li>
    <li><b>Fixation does not occur</b>. Instead, the chain converges to a <b>steady-state distribution</b> over the state space {0, ..., 2N}.</li>
  </ul>

  <h3>3. Adding Selection</h3>
  <p>
    To model <b>natural selection</b>, we suppose that a-genes have a reproductive advantage.
  </p>
  <p>
    If a-genes have a selective advantage <b>s</b>, then:
    </p>
    <ul>
      <li>Each a-gene reproduces with probability proportional to <b>(1 + s)</b></li>
      <li>Each A-gene reproduces with probability proportional to 1</li>
    </ul>

  <h4>Adjusted Probability of Choosing a-gene:</h4>
  <p>
    If there are j a-genes and (2N − j) A-genes, then total fitness = <b>(1 + s)j + (2N − j)</b>
  </p>
  <p>
    So,
    <br><b>p<sub>j</sub> = [(1 + s)j] / [(1 + s)j + (2N − j)]</b>,
    <br><b>q<sub>j</sub> = 1 − p<sub>j</sub></b>
  </p>

  <h4>Biological Meaning:</h4>
  <ul>
    <li>If s > 0, then a-genes are more likely to be passed on.</li>
    <li>The population will tend to drift toward all a-genes (state 2N), even if started with a mix.</li>
    <li>This models <b>directional selection</b>.</li>
  </ul>

<h2>3(d). Discrete Queueing Markov Chain</h2>

  <h3>1. System Overview</h3>
  <p>
    This is a model of a <strong>queueing system</strong> where:
  </p>
  <ul>
    <li>Customers arrive randomly during each time period.</li>
    <li>At most one customer is served in each period (if someone is present).</li>
    <li>If the queue is empty, no service occurs in that period.</li>
  </ul>
  <p>
    A real-world example: A taxi stand where taxis arrive at fixed intervals. If no passengers are waiting, the taxi leaves immediately.
  </p>

  <h3>2. Customer Arrival Process</h3>
  <p>
    Let <strong>A<sub>n</sub></strong> be the number of new customers arriving in period n.
  </p>
  <ul>
    <li>The number of arrivals follows a probability distribution:
      <br><strong>Pr(A<sub>n</sub> = k) = a<sub>k</sub></strong>, for k = 0, 1, 2, ...
    </li>
    <li>The arrivals in each period are <strong>independent and identically distributed (i.i.d.)</strong>.</li>
    <li>The probabilities satisfy:
      <br><strong>Σ a<sub>k</sub> = 1</strong>, and <strong>a<sub>k</sub> ≥ 0</strong>.
    </li>
  </ul>

  <h3>3. State of the System</h3>
  <p>
    Define <strong>X<sub>n</sub></strong> as the number of customers in the queue at the <strong>start</strong> of period n.
  </p>
  <p>
    During a period:
  </p>
  <ul>
    <li>If <strong>X<sub>n</sub> ≥ 1</strong>: One customer is served, and <strong>A<sub>n</sub></strong> customers arrive.</li>
    <li>If <strong>X<sub>n</sub> = 0</strong>: No service happens, but still <strong>A<sub>n</sub></strong> customers may arrive.</li>
  </ul>

  <h4>Update Rule:</h4>
  <p>
    The number of customers at the start of next period is:
    <br><strong>X<sub>n+1</sub> = max(0, X<sub>n</sub> − 1) + A<sub>n</sub></strong>
  </p>
  <p>
    This captures the logic: one customer leaves (if present), and A<sub>n</sub> arrive.
  </p>

  <h3>4. Transition Probability Matrix</h3>
  <p>
    Let’s denote the transition probability <strong>P<sub>ij</sub></strong> = Pr(X<sub>n+1</sub> = j | X<sub>n</sub> = i)
  </p>

  <h4>If i = 0 (no one is in queue):</h4>
  <ul>
    <li>No one is served, so the new state is just A<sub>n</sub>.</li>
    <li><strong>P<sub>0k</sub> = a<sub>k</sub></strong></li>
  </ul>

  <h4>If i ≥ 1 (at least one customer in queue):</h4>
  <ul>
    <li>One customer is served, so current queue becomes (i − 1).</li>
    <li>A<sub>n</sub> new arrivals → New state = i − 1 + A<sub>n</sub></li>
    <li><strong>P<sub>ij</sub> = a<sub>j − (i − 1)</sub></strong>, only if j ≥ i − 1</li>
  </ul>

  <h4>Example Matrix:</h4>
  <table border="1" cellpadding="5">
    <tr><th>From/To</th><th>0</th><th>1</th><th>2</th><th>3</th><th>4</th></tr>
    <tr><td>0</td><td>a<sub>0</sub></td><td>a<sub>1</sub></td><td>a<sub>2</sub></td><td>a<sub>3</sub></td><td>a<sub>4</sub></td></tr>
    <tr><td>1</td><td>0</td><td>a<sub>0</sub></td><td>a<sub>1</sub></td><td>a<sub>2</sub></td><td>a<sub>3</sub></td></tr>
    <tr><td>2</td><td>0</td><td>0</td><td>a<sub>0</sub></td><td>a<sub>1</sub></td><td>a<sub>2</sub></td></tr>
    <tr><td>3</td><td>0</td><td>0</td><td>0</td><td>a<sub>0</sub></td><td>a<sub>1</sub></td></tr>
  </table>

  <h3>5. Long-term Behavior</h3>
  <h4>Expected Number of Arrivals:</h4>
  <p>
    Let the expected number of new customers in a period be:
    <br><strong>λ = Σ k * a<sub>k</sub></strong>
  </p>

  <h4>Case 1: λ &lt; 1 (stable system)</h4>
  <ul>
    <li>Queue reaches a <strong>statistical equilibrium</strong>.</li>
    <li>A limiting distribution π = {π<sub>k</sub>} exists such that:
      <br><strong>lim<sub>n→∞</sub> Pr(X<sub>n</sub> = k | X<sub>0</sub> = j) = π<sub>k</sub></strong>
    </li>
  </ul>

  <h4>Case 2: λ ≥ 1 (unstable system)</h4>
  <ul>
    <li>The queue size increases indefinitely over time.</li>
    <li>No steady-state distribution exists.</li>
  </ul>

  <h3>6. Important Metrics</h3>
  <h4>1. Long-run Idle Time:</h4>
  <p>
    The system is idle (no customer) only in state 0.
    <br><strong>Fraction of time idle = π<sub>0</sub></strong>
  </p>

  <h4>2. Mean Number of Customers in System:</h4>
  <p>
    <strong>L = Σ (k * π<sub>k</sub>)</strong>
  </p>

  <h4>3. Long-run Mean Time a Customer Spends in the System (Little’s Law):</h4>
  <p>
    Let T be the expected time spent in system.
    <br><strong>T = L / λ</strong>, where λ is the arrival rate
  </p>
  <h2>4 First Step Analysis</h2>

<p>
A surprising number of problems involving Markov chains can be solved using a technique called <strong>first step analysis</strong>. This technique works by analyzing what happens during the first step (or transition) of the Markov process and then using the <strong>law of total probability</strong> and the <strong>Markov property</strong> to build equations that involve unknown quantities of interest. 
</p>

<h3>4.1 Simple First Step Analyses</h3>

<p>
Consider a Markov chain with three states: 0, 1, and 2. The transition probability matrix is structured as follows:
</p>

<ul>
  <li>From state 0: it always stays in state 0 (absorbing)</li>
  <li>From state 1: with probability <em>α</em>, it moves to state 0; with probability <em>β</em>, it stays in state 1; and with probability <em>γ</em>, it moves to state 2</li>
  <li>From state 2: it always stays in state 2 (absorbing)</li>
</ul>

<p>
Here, α + β + γ = 1, and α, β, γ are all greater than 0.
</p>

<p>
So, if the chain starts in state 1, it may stay there for a while but will eventually get absorbed into either state 0 or state 2.
</p>

<p>
There are two natural questions to ask:
</p>

<ol>
  <li>What is the probability that the process eventually ends up in state 0 (instead of 2)?</li>
  <li>How long, on average, does it take for the process to get absorbed into either state 0 or state 2?</li>
</ol>

<p>
To answer these questions, we use first step analysis.
</p>

<h4>Defining Variables</h4>

<p>
Let <strong>T</strong> be the time (step number) when the process is absorbed (enters state 0 or 2 for the first time).
</p>

<p>
Let <strong>u</strong> be the probability that the process is eventually absorbed in state 0, given it starts in state 1:
</p>

<p><em>u = P[X_T = 0 | X_0 = 1]</em></p>

<p>
Let <strong>v</strong> be the expected number of steps until absorption, again given that the process starts in state 1:
</p>

<p><em>v = E[T | X_0 = 1]</em></p>

<h4>Using First Step Analysis to Compute u</h4>

<p>
We break the analysis based on where the process goes in its first step from state 1:
</p>

<ul>
  <li>If it moves to state 0 (probability α), then T = 1 and X_T = 0</li>
  <li>If it moves to state 2 (probability γ), then T = 1 and X_T = 2</li>
  <li>If it stays in state 1 (probability β), the situation is the same as before — the process restarts from state 1</li>
</ul>

<p>
So the probability of eventually reaching state 0 from state 1, denoted u, satisfies:
</p>

<p>
u = α × 1 + β × u + γ × 0
</p>

<p>
Simplifying:
</p>

<p>
u = α + βu
</p>

<p>
Solving for u:
</p>

<p>
u - βu = α → u(1 - β) = α → u = α / (1 - β)
</p>

<p>
This quantity represents the conditional probability of eventual absorption into state 0, assuming the process starts in state 1.
</p>

<h4>Using First Step Analysis to Compute v</h4>

<p>
Now, we calculate v, the expected number of steps until absorption.
</p>

<p>
Again we break into cases:
</p>

<ul>
  <li>If the process moves to state 0 or 2 (probability α + γ), it takes exactly 1 step</li>
  <li>If it stays in state 1 (probability β), then it takes 1 step now, and on average another v steps in the future (since it returns to where it started)</li>
</ul>

<p>
So:
</p>

<p>
v = α × 1 + β × (1 + v) + γ × 1
</p>

<p>
v = (α + γ) × 1 + β(1 + v)
</p>

<p>
v = (1 - β) + β(1 + v)
</p>

<p>
Simplify:
</p>

<p>
v = 1 + βv → v(1 - β) = 1 → v = 1 / (1 - β)
</p>

<h4>Verification via Geometric Distribution</h4>

<p>
The absorption time T, starting from state 1, actually follows a geometric distribution:
</p>

<p>
P[T > k | X_0 = 1] = β^k for k = 0, 1, 2, ...
</p>

<p>
This means:
</p>

<p>
E[T | X_0 = 1] = ∑ (from k = 0 to ∞) β^k = 1 / (1 - β)
</p>

<p>
This matches the result we obtained from first step analysis. In this simple case, the result can be verified directly, but for more complex Markov chains, first step analysis is often the only feasible method.
</p>
<h3>Extension to a Four-State Markov Chain</h3>

<p>
Now we consider a more complex situation: a four-state Markov chain with states 0, 1, 2, and 3. The transition matrix is given by:
</p>

<ul>
  <li>State 0 and State 3 are <strong>absorbing states</strong> (once the process enters them, it stays there forever).</li>
  <li>States 1 and 2 are <strong>transient states</strong> (the process can leave them).</li>
</ul>

<p>
The general form of the transition matrix is:
</p>

<pre>
    0   1   2   3
0 [ 1   0   0   0 ]
1 [P10 P11 P12 P13]
2 [P20 P21 P22 P23]
3 [ 0   0   0   1 ]
</pre>

<p>
Because absorption occurs in state 0 or 3, and the initial state could be either 1 or 2 (both transient), the probability of absorption in state 0 depends on where the process starts. To address this, we define the following:
</p>

<ul>
  <li><strong>T</strong> = first time the process reaches either state 0 or 3</li>
  <li><strong>u<sub>i</sub></strong> = probability that the process is absorbed in state 0, given that it started in state <em>i</em> (i = 1, 2)</li>
  <li><strong>v<sub>i</sub></strong> = expected time until absorption, given that it started in state <em>i</em> (i = 1, 2)</li>
</ul>

<p>
We also define for consistency:
</p>

<ul>
  <li><strong>u<sub>0</sub> = 1</strong> (since it is already in state 0)</li>
  <li><strong>u<sub>3</sub> = 0</strong> (since it's already in state 3 and never goes to 0)</li>
  <li><strong>v<sub>0</sub> = 0</strong> and <strong>v<sub>3</sub> = 0</strong> (already absorbed)</li>
</ul>

<h4>First Step Analysis for u<sub>1</sub> and u<sub>2</sub></h4>

<p>
Suppose the process starts in state 1. The first step could result in:
</p>

<ul>
  <li>Moving to state 0: contributes <strong>P10 × 1</strong></li>
  <li>Moving to state 1: contributes <strong>P11 × u<sub>1</sub></strong> (we're back where we started)</li>
  <li>Moving to state 2: contributes <strong>P12 × u<sub>2</sub></strong></li>
</ul>

<p>
This leads to the equation:
</p>

<p><strong>u<sub>1</sub> = P10 + P11 × u<sub>1</sub> + P12 × u<sub>2</sub></strong> (Equation 3.21)</p>

<p>
Similarly, for state 2:
</p>

<p><strong>u<sub>2</sub> = P20 + P21 × u<sub>1</sub> + P22 × u<sub>2</sub></strong> (Equation 3.22)</p>

<p>
These two equations can be solved simultaneously to find the values of u<sub>1</sub> and u<sub>2</sub>.
</p>

<h4>Numerical Example</h4>

<p>
Consider the specific transition matrix:
</p>

<pre>
    0    1    2    3
0 [ 1    0    0    0 ]
1 [0.4  0.3  0.2  0.1]
2 [0.1  0.3  0.3  0.3]
3 [0    0    0    1 ]
</pre>

<p>
Substituting into the equations for u<sub>1</sub> and u<sub>2</sub>:
</p>

<p>
u<sub>1</sub> = 0.4 + 0.3 × u<sub>1</sub> + 0.2 × u<sub>2</sub><br>
u<sub>2</sub> = 0.1 + 0.3 × u<sub>1</sub> + 0.3 × u<sub>2</sub>
</p>

<p>
Solving the system:
</p>

<pre>
0.7u<sub>1</sub> + 0.2u<sub>2</sub> = 0.4
0.3u<sub>1</sub> + 0.7u<sub>2</sub> = 0.1
</pre>

<p>
The solution is:
</p>

<ul>
  <li><strong>u<sub>1</sub> = 30/43</strong></li>
  <li><strong>u<sub>2</sub> = 19/43</strong></li>
</ul>

<p>
So, starting from state 2, the probability that the process is eventually absorbed in state 0 is 19/43, and in state 3 is 24/43 (since the total must sum to 1).
</p>

<h4>Mean Time to Absorption (v<sub>1</sub>, v<sub>2</sub>)</h4>

<p>
We now use first step analysis to find the expected number of steps until absorption, starting from state 1 or 2.
</p>

<p>
If we start from state 1:
</p>

<ul>
  <li>With probability P11, it returns to state 1 → additional v<sub>1</sub> steps expected</li>
  <li>With probability P12, it goes to state 2 → additional v<sub>2</sub> steps expected</li>
</ul>

<p>
The equation becomes:
</p>

<p><strong>v<sub>1</sub> = 1 + P11 × v<sub>1</sub> + P12 × v<sub>2</sub></strong></p>

<p>
Similarly, for state 2:
</p>

<p><strong>v<sub>2</sub> = 1 + P21 × v<sub>1</sub> + P22 × v<sub>2</sub></strong></p>

<p>
Substituting the numerical values:
</p>

<p>
v<sub>1</sub> = 1 + 0.3 × v<sub>1</sub> + 0.2 × v<sub>2</sub><br>
v<sub>2</sub> = 1 + 0.3 × v<sub>1</sub> + 0.3 × v<sub>2</sub>
</p>

<p>
Solving these:
</p>

<pre>
0.7v<sub>1</sub> - 0.2v<sub>2</sub> = 1
-0.3v<sub>1</sub> + 0.7v<sub>2</sub> = 1
</pre>

<p>
The solutions are:
</p>

<ul>
  <li><strong>v<sub>1</sub> = (90/43) ≈ 2.09</strong></li>
  <li><strong>v<sub>2</sub> = (100/43) ≈ 2.33</strong></li>
</ul>

<p>
Therefore, if the process starts from state 2, it will take about 2.33 steps, on average, before it gets absorbed into either state 0 or 3.
</p>
<h3>4.2 General Structure of Absorbing Markov Chains</h3>
<p>
Consider a finite-state Markov chain \( \{X_n\} \) with state space labeled \( 0, 1, \ldots, N \).
Assume states \( 0, 1, \ldots, r-1 \) are <strong>transient</strong>, and states \( r, r+1, \ldots, N \)
are <strong>absorbing</strong>.
</p>
<p>
The transition matrix \( P \) has the following block form:
</p>
\[
P =
\begin{bmatrix}
Q & R \\
0 & I
\end{bmatrix}
\]
<p>
Here, \( Q \) governs transitions among transient states, \( R \) represents transitions from transient to absorbing states, 
\( 0 \) is a zero matrix, and \( I \) is the identity matrix for absorbing states.
</p>

<h3>Probability of Absorption in a Given State</h3>
<p>
Let \( u_i^{(k)} = \mathbb{P}( \text{absorption in state } k \mid X_0 = i ) \), for \( i < r \), 
where \( k \geq r \) is a fixed absorbing state. A first-step analysis gives:
</p>
\[
u_i^{(k)} = P_{ik} + \sum_{j=0}^{r-1} P_{ij} u_j^{(k)}
\]
<p>
This yields a system of linear equations for \( u_0^{(k)}, u_1^{(k)}, \ldots, u_{r-1}^{(k)} \).
</p>

<h3>Example: A Rat in a Maze</h3>

<div style="text-align: center; margin: 20px 0;">
  <img src="rat.png" alt="Rat in a maze" width="600">
</div>

<p>
A rat moves randomly through a maze of 9 compartments. Compartments 7 (food) and 8 (shock) are absorbing.
The transition probabilities are determined by uniform movement to adjacent compartments.
Let \( u_i \) denote the probability that the rat reaches food (state 7) before shock (state 8), given that it starts at state \( i \).
</p>
<p>
Using first-step analysis, the equations are:
</p>
\[
\begin{aligned}
u_0 &= \frac{1}{2}u_1 + \frac{1}{2}u_2 \\
u_1 &= \frac{1}{3}u_0 + \frac{1}{3}u_3 + \frac{1}{3} \\
u_2 &= \frac{1}{3}u_0 + \frac{1}{3}u_3 \\
u_3 &= \frac{1}{4}u_1 + \frac{1}{4}u_2 + \frac{1}{4}u_4 + \frac{1}{4}u_5 \\
u_4 &= \frac{1}{3}u_3 + \frac{1}{3}u_6 \\
u_5 &= \frac{1}{3}u_3 + \frac{1}{3}u_6 \\
u_6 &= \frac{1}{2}u_4 + \frac{1}{2}u_5 \\
u_7 &= 1 \\
u_8 &= 0
\end{aligned}
\]
<p>
By symmetry:
</p>
<ul>
  <li>\( u_0 = u_6 \)</li>
  <li>\( u_1 = u_4 \)</li>
  <li>\( u_2 = u_5 \)</li>
  <li>\( u_3 = \frac{1}{2} \)</li>
</ul>
<p>
This simplifies the system to:
</p>
\[
\begin{aligned}
u_0 &= \frac{1}{2}u_1 + \frac{1}{2}u_2 \\
u_1 &= \frac{1}{3}u_0 + \frac{1}{3} \cdot \frac{1}{2} + \frac{1}{3} \\
u_2 &= \frac{1}{3}u_0 + \frac{1}{3} \cdot \frac{1}{2}
\end{aligned}
\]
<p>
Solving these gives:
</p>
<ul>
  <li>\( u_0 = \frac{1}{2} \)</li>
  <li>\( u_1 = \frac{2}{3} \)</li>
  <li>\( u_2 = \frac{1}{3} \)</li>
</ul>

<h3>Expected Time Until Absorption</h3>
<p>
Define the absorption time:
</p>
\[
T = \min \{ n \geq 0 \mid X_n \geq r \}
\]
<p>
Let each transient state \( i \) have an associated rate \( g(i) \). Define:
</p>
\[
w_i = \mathbb{E} \left[ \sum_{n=0}^{T-1} g(X_n) \mid X_0 = i \right]
\]
<p>
This satisfies the equation:
</p>
\[
w_i = g(i) + \sum_{j=0}^{r-1} P_{ij} w_j
\]

<h4>Special Case 1: Mean Time to Absorption</h4>
<p>
If \( g(i) = 1 \) for all \( i \), then:
</p>
\[
v_i = \mathbb{E}[T \mid X_0 = i] = 1 + \sum_{j=0}^{r-1} P_{ij} v_j
\]

<h4>Special Case 2: Expected Visits to State \( k \)</h4>
<p>
If \( g(i) = \delta_{ik} \), then:
</p>
\[
W_{ik} = \mathbb{E}[\text{number of visits to } k \mid X_0 = i] = \delta_{ik} + \sum_{j=0}^{r-1} P_{ij} W_{jk}
\]
<h2>5 Some Special Markov Chains</h2>

<p>
We introduce several particular Markov chains that arise in a variety of applications.
</p>

<h3>5.1 The Two-State Markov Chain</h3>

<p>
Let the transition matrix be given by:
</p>

\[
P =
\begin{bmatrix}
1 - a & a \\
b & 1 - b
\end{bmatrix}
\quad \text{where } 0 &lt; a, b &lt; 1 \tag{3.30}
\]

<p>
When \( a = 1 - b \), so that the rows of \( P \) are the same, then the states \( X_1, X_2, \dots \) are 
independent identically distributed (i.i.d.) random variables with \( \Pr(X_n = 0) = b \) and \( \Pr(X_n = 1) = a \). 
When \( a \ne 1 - b \), the probability distribution for \( X_n \) depends on the outcome \( X_{n-1} \) at the previous stage.
</p>

<p>
For the two-state Markov chain, it can be verified by induction that the <em>n</em>-step transition matrix is given by:
</p>
\[
P^n = \frac{1}{a + b} \left(
\begin{bmatrix}
b & a \\
b & a
\end{bmatrix}
+ (1 - a - b)^n
\begin{bmatrix}
a & -a \\
-b & b
\end{bmatrix}
\right) \tag{3.31}
\]

<p>
To verify this formula, define the matrices:
</p>

\[
A = 
\begin{bmatrix}
b & a \\
b & a
\end{bmatrix}, \quad
B = 
\begin{bmatrix}
a & -a \\
-b & b
\end{bmatrix}
\]

<p>
Then equation (3.31) becomes:
</p>

\[
P^n = \frac{1}{a + b} \left( A + (1 - a - b)^n B \right)
\]

<p>
Now check the matrix multiplications:
</p>

\[
AP =
\begin{bmatrix}
b & a \\
b & a
\end{bmatrix}
\begin{bmatrix}
1 - a & a \\
b & 1 - b
\end{bmatrix}
=
\begin{bmatrix}
b & a \\
b & a
\end{bmatrix}
= A
\]

\[
BP =
\begin{bmatrix}
a & -a \\
-b & b
\end{bmatrix}
\begin{bmatrix}
1 - a & a \\
b & 1 - b
\end{bmatrix}
=
(1 - a - b) B
\]

<p>
Hence, for \( n = 1 \), we have:
</p>

\[
P^1 = \frac{1}{a + b} \left( A + (1 - a - b) B \right) = P
\]

<p>
To complete the induction, assume the formula holds for \( n \). Then:
</p>

\[
P^{n+1} = P^n P = \frac{1}{a + b} \left( A + (1 - a - b)^n B \right) P = \frac{1}{a + b} \left( A + (1 - a - b)^{n+1} B \right)
\]

<p>
This confirms the inductive step. Since \( 0 &lt; a + b &lt; 1 \), it follows that \( (1 - a - b)^n \to 0 \) as \( n \to \infty \), and thus:
</p>

\[
\lim_{n \to \infty} P^n = \frac{1}{a + b}
\begin{bmatrix}
b & a \\
b & a
\end{bmatrix}
\]

<h4>Numerical Example</h4>

<p>
Suppose the items produced by a certain worker are graded as defective or not. Due to trends in raw material quality, 
whether or not a particular item is defective depends in part on whether or not the previous item was defective.
Let \( X_n \) denote the quality of the \( n \)th item, with \( X_n = 0 \) meaning "good" and \( X_n = 1 \) meaning "defective."
</p>

<p>
Suppose \( X_n \) evolves as a Markov chain whose transition matrix is:
</p>

\[
P =
\begin{bmatrix}
0.99 & 0.01 \\
0.12 & 0.88
\end{bmatrix}
\]

<p>
Defective items tend to appear in bunches in the output of such a system. 
In the long run, the probability that an item is defective is:
</p>

\[
\frac{a}{a + b} = \frac{0.01}{0.01 + 0.12} = 0.077
\]
<h3>5.2 Markov Chains Defined by Independent Random Variables</h3>

<p>
Let \( \xi \) be a discrete random variable taking nonnegative integer values with
\( \Pr(\xi = i) = a_i \), for \( i = 0, 1, 2, \dots \), where \( \sum_i a_i = 1 \). Let \( \xi_1, \xi_2, \dots \) be independent copies of \( \xi \). We now define three different Markov chains associated with this sequence.
</p>

<h4>Example 1: Independent Random Variables</h4>

<p>
Define a process \( X_n = \xi_n \), with \( X_0 = 0 \). The transition matrix is:
</p>

\[
P =
\begin{bmatrix}
a_0 & a_1 & a_2 & \cdots \\
a_0 & a_1 & a_2 & \cdots \\
a_0 & a_1 & a_2 & \cdots \\
\vdots & \vdots & \vdots & \ddots
\end{bmatrix}
\tag{3.33}
\]

<p>
All rows are identical, expressing the fact that \( X_{n+1} \) is independent of \( X_n \).
</p>

<h4>Example 2: Successive Maxima</h4>

<p>
Define \( X_n = \max(\xi_1, \xi_2, \dots, \xi_n) \), with \( X_0 = 0 \). This process is Markovian since:
</p>

\[
X_{n+1} = \max(X_n, \xi_{n+1})
\]

<p>
Define \( A_k = a_0 + a_1 + \cdots + a_k \). Then the transition matrix is:
</p>

\[
P =
\begin{bmatrix}
A_0 & a_1 & a_2 & a_3 & \cdots \\
0 & A_1 & a_2 & a_3 & \cdots \\
0 & 0 & A_2 & a_3 & \cdots \\
0 & 0 & 0 & A_3 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{bmatrix}
\tag{3.34}
\]

<p>
This model is useful, for example, in auction theory. If bids \( \xi_1, \xi_2, \dots \) are made on an asset, and the item is sold the first time the bid exceeds a threshold \( M \), then the time of sale is:
</p>

\[
T = \min\{n \geq 1 \mid X_n \geq M\}
\]

<p>
Using first-step analysis, the expected value of \( T \) is:
</p>

\[
\mathbb{E}[T] = \frac{1}{\Pr(\xi_1 \geq M)} = \frac{1}{a_M + a_{M+1} + \cdots}
\tag{3.35}
\]

<h4>Example 3: Partial Sums</h4>

<p>
Define the cumulative sums \( S_n = \xi_1 + \cdots + \xi_n \), with \( S_0 = 0 \). Then the process \( X_n = S_n \) is a Markov chain because:
</p>

\[
\Pr(X_{n+1} = j \mid X_n = i) = \Pr(\xi_{n+1} = j - i) = a_{j - i}, \quad \text{for } j \geq i
\]

<p>
So the transition matrix has the form:
</p>

\[
P =
\begin{bmatrix}
a_0 & a_1 & a_2 & a_3 & \cdots \\
0 & a_0 & a_1 & a_2 & \cdots \\
0 & 0 & a_0 & a_1 & \cdots \\
0 & 0 & 0 & a_0 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{bmatrix}
\tag{3.37}
\]

<p>
If \( \xi \) can take both positive and negative values, then \( S_n \) takes values over all integers, and the state space becomes \( \mathbb{Z} \). The transition matrix becomes symmetric:
</p>

\[
P =
\begin{bmatrix}
\cdots & a_2 & a_1 & a_0 & a_1 & a_2 & \cdots \\
\cdots & a_3 & a_2 & a_1 & a_0 & a_1 & \cdots \\
\cdots & a_4 & a_3 & a_2 & a_1 & a_0 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \ddots
\end{bmatrix}
\]

<p>
Here, \( \Pr(\xi = k) = a_k \), for \( k \in \mathbb{Z} \), and \( \sum_k a_k = 1 \). The symmetry of the transition matrix reflects the symmetry of the support of \( \xi \).
</p>

<h4>Example 4: One-Dimensional Random Walks</h4>

<p>
When discussing random walks, it is helpful to think of the system's state as the position of a moving "particle."
A one-dimensional random walk is a Markov chain with a state space that is a finite or infinite subset of the integers. If the particle is in state \( i \), it can, in a single transition, either stay in \( i \) or move to one of the neighboring states \( i + 1 \) or \( i - 1 \).
</p>

<p>
If the state space is taken as the non-negative integers, the transition matrix has the form:
</p>

\[
P =
\begin{array}{c|cccccc}
 & 0 & 1 & 2 & \cdots & i-1 & i+1 \\
\hline
0 & r_0 & p_0 & 0 & \cdots & 0 & 0 \\
1 & q_1 & r_1 & p_1 & \cdots & 0 & 0 \\
2 & 0 & q_2 & r_2 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
i & 0 & 0 & 0 & \cdots & q_i & r_i & p_i \\
\end{array}
\]

<p>
Here, \( p_i > 0 \), \( q_i > 0 \), \( r_i \geq 0 \), and for \( i \geq 1 \), we have:
</p>

\[
q_i + r_i + p_i = 1
\]

<p>
At the boundary state \( i = 0 \), assume:
</p>

\[
p_0 > 0, \quad r_0 \geq 0, \quad q_0 = 0, \quad \text{and} \quad r_0 + p_0 = 1
\]

<p>
Specifically, if \( X_n = i \), then for \( i \geq 1 \):
</p>

\[
\begin{aligned}
\Pr(X_{n+1} = i + 1 \mid X_n = i) &= p_i \\
\Pr(X_{n+1} = i - 1 \mid X_n = i) &= q_i \\
\Pr(X_{n+1} = i \mid X_n = i) &= r_i
\end{aligned}
\]

<p>
Appropriate modifications are made for the boundary case \( i = 0 \).
The term “random walk” is fitting, as a realization of the process resembles the path of a person (suitably intoxicated) moving randomly one step forward or backward.
</p>

<h4>Example 5: Gambler’s Ruin and Random Walks</h4>

<p>
The fortune of a player engaged in a sequence of contests can be modeled as a random walk process.
Suppose a player \( A \), with current fortune \( k \), plays a game against an infinitely rich adversary. Let \( p_k \) be the probability of winning one unit and \( q_k = 1 - p_k \) the probability of losing one unit in the next contest. The process \( X_n \), representing the fortune after \( n \) games, is a Markov chain.
</p>

<p>
Once state 0 is reached (i.e., player \( A \) is wiped out), the process stays at 0. This event is known as <strong>gambler’s ruin</strong>.
</p>

<p>
Now suppose both players \( A \) and \( B \) have finite fortunes summing to \( N \), and player \( A \) starts with fortune \( k \). The state space is \( \{0, 1, 2, \dots, N\} \), where \( X_n \) is player \( A \)'s fortune at time \( n \), and \( N - X_n \) is player \( B \)'s.
</p>

<p>
If we allow the possibility of a draw, the transition matrix becomes:
</p>

\[
P =
\begin{array}{c|cccccc}
 & 0 & 1 & 2 & \cdots & N-1 & N \\
\hline
0 & 1 & 0 & 0 & \cdots & 0 & 0 \\
1 & q_1 & r_1 & p_1 & \cdots & 0 & 0 \\
2 & 0 & q_2 & r_2 & p_2 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
N-1 & 0 & 0 & \cdots & q_{N-1} & r_{N-1} & p_{N-1} \\
N & 0 & 0 & 0 & \cdots & 0 & 1 \\
\end{array}
\tag{3.39}
\]

<p>
When player \( A \)'s fortune hits 0 (ruin) or \( N \) (opponent's ruin), the process stays in that state forever.
</p>

<p>
In the special case where the contest probabilities are identical at every stage, i.e., \( p_k = p \), \( q_k = q = 1 - p \), and \( r_k = 0 \), the transition matrix simplifies to:
</p>

\[
P =
\begin{array}{c|cccccc}
 & 0 & 1 & 2 & \cdots & N-1 & N \\
\hline
0 & 1 & 0 & 0 & \cdots & 0 & 0 \\
1 & q & 0 & p & \cdots & 0 & 0 \\
2 & 0 & q & 0 & \cdots & 0 & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
N-1 & 0 & 0 & \cdots & 0 & q & p \\
N & 0 & 0 & 0 & \cdots & 0 & 1 \\
\end{array}
\tag{3.40}
\]

<p>
Let \( u_i \) denote the probability that player \( A \) is ruined (i.e., reaches state 0 before \( N \)) starting from initial fortune \( i \). Then:
</p>

\[
u_i = p u_{i+1} + q u_{i-1}, \quad \text{for } i = 1, 2, \dots, N-1
\tag{3.41}
\]

<p>
with boundary conditions:
</p>

\[
u_0 = 1, \quad u_N = 0
\]

<p>
Solving this recursion gives:
</p>

\[
u_i =
\begin{cases}
\frac{N - i}{N} & \text{if } p = q = \frac{1}{2} \\
\frac{(q/p)^i - (q/p)^N}{1-(q/p)^N} & \text{if } p \neq q
\end{cases}
\tag{3.42}
\]

<p>
These probabilities reflect the chances of gambler’s ruin based on initial fortune \( i \) and the fairness of the game. In a fair game, \( p = q \), the probability of ruin is \( u_i = 1 - \frac{i}{N} \). In a favorable game for player \( A \) (\( p > q \)), the ruin probability decreases exponentially with \( i \).
</p>

<p>
If player \( B \) is infinitely rich (i.e., \( N \to \infty \)), then:
</p>

\[
u_i =
\begin{cases}
1 & \text{if } p \leq q \\
\left( \frac{q}{p} \right)^i & \text{if } p > q
\end{cases}
\]

<p>
Hence, ruin is certain for player \( A \) if the game is fair or unfavorable. Only in a favorable game does player \( A \) have a chance to avoid ruin, and that chance increases with their initial fortune.
</p>
<h4>Example 6: Success Runs</h4>

<p>
Consider a Markov chain on the non-negative integers with the following transition probability matrix:
</p>

\[
P =
\begin{array}{c|ccccc}
 & 0 & 1 & 2 & 3 & \cdots \\
\hline
0 & p_0 & q_0 & 0 & 0 & \cdots \\
1 & p_1 & r_1 & q_1 & 0 & \cdots \\
2 & p_2 & 0 & r_2 & q_2 & \cdots \\
3 & p_3 & 0 & 0 & r_3 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \vdots & \ddots \\
\end{array}
\tag{3.44}
\]

<p>
Here, \( p_i > 0 \), \( q_i > 0 \), and \( p_i + q_i + r_i = 1 \) for \( i = 0, 1, 2, \dots \). The state 0 is special: it can be reached from any state in one transition, while state \( i+1 \) can only be reached from state \( i \).
</p>

<p>
This structure frequently appears in applications and is especially useful for illustrating concepts due to its computational simplicity.
</p>

<p>
A notable application arises in the context of <strong>success runs</strong> in repeated trials where each trial results in either a success (\( S \)) or failure (\( F \)). Suppose each trial has probability \( \theta \) of success and \( 1 - \theta \) of failure. A success run of length \( r \) is said to occur at trial \( n \) if the outcomes in the previous \( r - 1 \) trials followed by the current one were \( FSS\ldots S \), with \( r \) successive successes preceded by a failure.
</p>

<p>
We define the current state of the process to be the length of the ongoing success run. If the latest trial results in a failure, the state resets to 0. If there are \( r \) consecutive successes preceded by a failure, the state is \( r \). Because the trials are independent, the resulting process is Markovian.
</p>

<p>
In this case, the transition probabilities take the special form:
</p>

\[
p_n = \theta, \quad q_n = 1 - \theta, \quad r_n = 0, \quad \text{for } n \geq 0
\]

<p>
Thus, the transition matrix simplifies to a form that directly reflects transitions based on success/failure outcomes in independent trials, modeling success run lengths efficiently.
</p>






  

























<p><strong>Last updated:</strong> June 5, 2025</p>
</section>

<div class="nav-bottom">
  <a href="tk-intro.html">Prev Chapter &larr;</a>
  <a href="tk-markov-chains2.html">Next Chapter &rarr;</a>
</div>

<footer>
  &copy; 2025 Mohammad Shaan | This site is maintained on GitHub Pages.
</footer>
<!-- MathJax for LaTeX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  function toggleHint(id) {
    var hint = document.getElementById(id);
    if (hint.style.display === "none" || hint.style.display === "") {
      hint.style.display = "block";
    } else {
      hint.style.display = "none";
    }
  }
</script>

</body>
</html>
