<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Stochastic Processes & Finance – Summer Reading Project</title>
  <link href="https://fonts.googleapis.com/css2?family=Lato&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Lato', sans-serif;
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      color: #333;
    }
    header {
      background-color: #003366;
      color: white;
      padding: 1em;
      text-align: center;
    }
    nav {
      background-color: #e0e0e0;
      padding: 1em;
      text-align: center;
    }
    nav a {
      margin: 0 1em;
      text-decoration: none;
      color: #003366;
      font-weight: bold;
    }
    section {
      max-width: 1200px;
      margin: 2em auto;
      padding: 1em;
      background: white;
      border-radius: 8px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }

    .nav-bottom a {
      flex: 1;
      text-align: center;
      padding: 0.5em 1em;
      color: #003366;
      font-weight: bold;
      text-decoration: none;
    }

    footer {
      text-align: center;
      font-size: 0.9em;
      color: #777;
      margin: 2em 0;
    }
    .hint-text {
      display: none;
      margin-top: 0.5em;
      background: #f0f0f0;
      padding: 0.5em;
      border-left: 4px solid #0077cc;
      border-radius: 4px;
    }

    .hint-button {
      margin-top: 1em;
      background-color: #0077cc;
      color: white;
      border: none;
      padding: 0.4em 0.8em;
      border-radius: 4px;
      cursor: pointer;
      font-size: 0.9em;
    }

    .hint-button:hover {
      background-color: #005fa3;
    }
  </style>
</head>
<body>

<header>
  <h1>Chapter 2: Markov Chains: Introduction</h1>
  <p>Study notes and chapter summaries</p>
</header>

<nav>
  <a href="index.html">Home</a>
  <a href="topics.html">Topics</a>
  <a href="notes.html">Notes</a>
  <a href="projects.html">Programming</a>
</nav>
<section>
<h2>1. Definition</h2>
<p>
  A Markov process \(X_t\) is a stochastic process with the property that, given the value of \(X_t\), the values of \(X_s\) for s>t are not influenced by the values of \(X_u\)  for \(t> u\).  
</p>
<p>
  A <strong>discrete-time Markov chain </strong> is a Markov process whose state space is a finite or countable set, and whose (time) index set is T =\( (0,1,2 \ldots ) \). Mathematically:
  $$ P(X_{n+1} = j| X_0 = i_0, \ldots , X_{n-1} = i_{n-1}, X_n= i) $$
  $$ = P( X_{n+1} =j | X_n= i) $$
  for all n and all states \( i_0 , \ldots, i_{n-1}, i, j \)
</p>
<h3>Understanding One-Step Transition Probabilities</h3>
<p>
In a Markov chain, we usually label the state space using non-negative integers: 
<strong>0, 1, 2, ...</strong>. Unless stated otherwise, we follow this convention.
</p>
<p>
If the chain is in a certain state at time \( n \), we say \( X_n = i \).
The probability that it moves to state \( j \) at the next time step \( n+1 \),
given that it is currently in state \( i \), is called the 
<strong>one-step transition probability</strong>. It is denoted by:
</p>
\[
P^{(n,n+1)}_{ij} = \Pr(X_{n+1} = j \mid X_n = i)
\]
<p>
This notation indicates that the transition probabilities can depend on the current time \( n \),
as well as the current and next states \( i \) and \( j \).
</p>
<p>
If these probabilities are independent of time, we say that the Markov chain has 
<strong>stationary transition probabilities</strong>. In that case, we write:
</p>
\[
P_{ij} = \Pr(X_{n+1} = j \mid X_n = i)
\]
<p>
This means the probability of transitioning from state \( i \) to state \( j \)
remains the same at every step.
</p>
<p>
These probabilities \( P_{ij} \) are usually arranged in a 
<strong>transition matrix</strong> \( P \) as follows:
</p>
\[
P = 
\begin{bmatrix}
P_{00} & P_{01} & P_{02} & \cdots \\
P_{10} & P_{11} & P_{12} & \cdots \\
P_{20} & P_{21} & P_{22} & \cdots \\
\vdots & \vdots & \vdots & \ddots \\
\end{bmatrix}
\]
<p>
Each row corresponds to the current state \( i \), and each column represents the
probability of transitioning to state \( j \).
</p>
<h3>The Markov Transition Matrix</h3>

<p>
In a Markov chain, we represent the one-step transition probabilities using a matrix \( P \),
called the <strong>Markov matrix</strong> or <strong>transition probability matrix</strong>.
Each entry \( P_{ij} \) in this matrix represents the probability of moving from state \( i \)
to state \( j \) in one step.
</p>

<p>
That is,
\[
P_{ij} = \Pr(X_{n+1} = j \mid X_n = i)
\]
</p>

<p>
The \( i \)th row of \( P \) gives the full probability distribution of the next state \( X_{n+1} \)
given that the current state \( X_n = i \). In other words, the \( i \)th row shows where the process
might go from state \( i \), and with what probabilities.
</p>

<p>
If the number of states is finite (say, \( 0, 1, \ldots, r \)), then \( P \) is a finite square matrix 
of order \( r + 1 \), meaning it has \( r+1 \) rows and \( r+1 \) columns.
</p>

<h4>Conditions for Valid Transition Matrices</h4>

<p>
The entries of the transition matrix must satisfy two conditions:
</p>

<ul>
  <li>
    <strong>Non-negativity:</strong> All entries must be non-negative: 
    \[
    P_{ij} \geq 0 \quad \text{for all } i, j
    \]
  </li>
  <li>
    <strong>Row sums equal to 1:</strong> The probabilities in each row must sum to 1:
    \[
    \sum_{j=0}^{r} P_{ij} = 1 \quad \text{for all } i
    \]
  </li>
</ul>

<p>
The second condition simply reflects the fact that <em>some</em> transition must occur at each step. 
Even if the state doesn't change (i.e., it stays in the same state), we still consider that a 
transition has occurred.
</p>
<h3>Theorem: A Markov process is completely defined once we specify its Markov matrix P and its initial state \( X_0 \)</h3>
<p> Let P{ \( X_0 = i \)} = \(p_{i_0}\). It's enough to show how to compute the probability P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_n= i_n \) }, since any probability containing \(X_{j_1}, X_{j_2}, \ldots, X_{j_k} , j_k> \ldots >j_2 >j_1  \) can be obtained by summing terms of the above form (by Theorem of Total Probability).
<p>P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_n= i_n \) } </p> 
<p>=P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_{n-1}= i_{n-1} \) } P{ \(X_n = i_n | X_0 = i_0, X_1 =i_1, \ldots , X_{n-1}= i_{n-1} \) }</p>
<p>=P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_{n-1}= i_{n-1} \) } P{ \(X_n = i_n | X_{n-1}=i_{n-1} \) } </p>
<p>=P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_{n-1}= i_{n-1} \) } \(P_{i_{n-1}i_n}\)</p>
<p>Similarly, P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_n= i_{n-1} \) }</p>
<p>=P{ \( X_0 = i_0, X_1 =i_1, \ldots , X_{n-2}= i_{n-2} \) } \(P_{i_{n-2}i_{n-1}}\)</p>
<p>Therefore, the original expression simplifies to:</p>
<p>
  \( P\left( X_0 = i_0, X_1 = i_1, \ldots , X_n = i_n \right) = p_{i_0} \cdot P_{i_0 i_1} \cdot P_{i_1 i_2} \cdot \ldots \cdot P_{i_{n-1} i_n} \)
</p>
<h2>2 Transition Probability Matrices of a Markov Chain</h2>

<p>
A Markov chain is fully defined by two things:
<ul>
  <li>Its one-step transition probability matrix \( P \)</li>
  <li>Its initial state distribution (i.e., the probabilities of starting in each state)</li>
</ul>
</p>

<p>
Most of the analysis of Markov chains revolves around calculating the probabilities of different outcomes after several steps or time periods. 
For this, we use the <strong>n-step transition probability matrix</strong>, denoted by \( P^{(n)} \).
</p>

<p>
The element \( P^{(n)}_{ij} \) represents the probability that the process moves from state \( i \) to state \( j \) in exactly \( n \) steps. Formally:
</p>

<p style="text-align: center;">
  \( P^{(n)}_{ij} = \Pr(X_{m+n} = j \mid X_m = i) \)
</p>

<p>
Since we are considering <em>stationary</em> (time-homogeneous) Markov chains, the probability depends only on the number of steps \( n \), and not on the starting time \( m \).
</p>

<p>
Thanks to the Markov property (which says that the future depends only on the present state, not the past), we can compute \( P^{(n)} \) recursively:
</p>

<p style="text-align: center;">
  \( P^{(n)}_{ij} = \sum_{k=0}^{\infty} P_{ik} \cdot P^{(n-1)}_{kj} \)
</p>

<p>
This recursive formula is just standard matrix multiplication, so we can write:
</p>

<p style="text-align: center;">
  \( P^{(n)} = P \cdot P^{(n-1)} \)
</p>

<p>
By repeatedly applying this formula (i.e., multiplying the matrix \( P \) with itself \( n \) times), we get:
</p>

<p style="text-align: center;">
  \( P^{(n)} = P^n \)
</p>

<p>
That is, the n-step transition probabilities are exactly the entries of the matrix \( P \) raised to the \( n \)-th power.
</p>

<p>
To complete the definition, we also define:
</p>

<ul>
  <li>\( P^{(0)}_{ij} = 1 \) if \( i = j \)</li>
  <li>\( P^{(0)}_{ij} = 0 \) if \( i \neq j \)</li>
</ul>

<p>
This just means that in zero steps, the process stays where it is.
</p>
<h3>Proof of the n-Step Transition Probability Formula</h3>

<p>
We want to prove the formula:
</p>

<p style="text-align: center;">
  \( P^{(n)}_{ij} = \sum_{k} P_{ik} \cdot P^{(n-1)}_{kj} \)
</p>

<p>
This formula means: the probability of going from state \( i \) to state \( j \) in \( n \) steps is the sum over all intermediate states \( k \) of the probability of:
</p>
<ol>
  <li>Going from \( i \) to \( k \) in one step: \( P_{ik} \), and</li>
  <li>Then from \( k \) to \( j \) in \( n-1 \) steps: \( P^{(n-1)}_{kj} \)</li>
</ol>

<p>
To see why this works, let’s look at what it means to reach \( j \) from \( i \) in exactly \( n \) steps.
</p>

<p>
We imagine that the first step takes the process from state \( i \) to some state \( k \). Then the remaining \( n - 1 \) steps must take the process from \( k \) to \( j \).
</p>

<p>
Now, because of the <strong>Markov property</strong>, once the process reaches \( k \), it "forgets" how it got there. So the probability of continuing from \( k \) to \( j \) depends only on \( k \), and not on any earlier states.
</p>

<p>
So, for each possible intermediate state \( k \), the probability of going from \( i \rightarrow k \rightarrow j \) in total \( n \) steps is:
</p>

<p style="text-align: center;">
  \( \Pr(X_1 = k \mid X_0 = i) \cdot \Pr(X_n = j \mid X_1 = k) = P_{ik} \cdot P^{(n-1)}_{kj} \)
</p>

<p>
To find the total probability of reaching \( j \) from \( i \) in \( n \) steps, we sum this product over all possible intermediate states \( k \):
</p>

<p style="text-align: center;">
  \( P^{(n)}_{ij} = \sum_{k} P_{ik} \cdot P^{(n-1)}_{kj} \)
</p>

<p>
This is exactly how matrix multiplication works, so we can write the matrix formula:
</p>

<p style="text-align: center;">
  \( P^{(n)} = P \cdot P^{(n-1)} \)
</p>

<p>
By repeating this step, we get:
</p>

<p style="text-align: center;">
  \( P^{(n)} = P^n \)
</p>

<p>
This completes the proof. It uses the basic idea that the path from \( i \) to \( j \) in \( n \) steps must go through some state \( k \) after the first step, and the rest follows by the Markov property and total probability.
</p>























<p><strong>Last updated:</strong> May 15, 2025</p>
</section>

<div class="nav-bottom">
  <a href="tk-intro.html">Prev Chapter &larr;</a>
  <a href="tk-markov-chains2.html">Next Chapter &rarr;</a>
</div>

<footer>
  &copy; 2025 Mohammad Shaan | This site is maintained on GitHub Pages.
</footer>
<!-- MathJax for LaTeX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  function toggleHint(id) {
    var hint = document.getElementById(id);
    if (hint.style.display === "none" || hint.style.display === "") {
      hint.style.display = "block";
    } else {
      hint.style.display = "none";
    }
  }
</script>

</body>
</html>
