<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Stochastic Processes & Finance ‚Äì Summer Reading Project</title>
  <link href="https://fonts.googleapis.com/css2?family=Lato&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Lato', sans-serif;
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      color: #333;
    }
    header {
      background-color: #003366;
      color: white;
      padding: 1em;
      text-align: center;
    }
    nav {
      background-color: #e0e0e0;
      padding: 1em;
      text-align: center;
    }
    nav a {
      margin: 0 1em;
      text-decoration: none;
      color: #003366;
      font-weight: bold;
    }
    section {
      max-width: 1200px;
      margin: 2em auto;
      padding: 1em;
      background: white;
      border-radius: 8px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }

    .nav-bottom a {
      flex: 1;
      text-align: center;
      padding: 0.5em 1em;
      color: #003366;
      font-weight: bold;
      text-decoration: none;
    }

    footer {
      text-align: center;
      font-size: 0.9em;
      color: #777;
      margin: 2em 0;
    }
    .hint-text {
      display: none;
      margin-top: 0.5em;
      background: #f0f0f0;
      padding: 0.5em;
      border-left: 4px solid #0077cc;
      border-radius: 4px;
    }

    .hint-button {
      margin-top: 1em;
      background-color: #0077cc;
      color: white;
      border: none;
      padding: 0.4em 0.8em;
      border-radius: 4px;
      cursor: pointer;
      font-size: 0.9em;
    }

    .hint-button:hover {
      background-color: #005fa3;
    }
  </style>
</head>
<body>

<header>
  <h1>Chapter 1: Introduction to Stochastic Processes</h1>
  <p>Study notes and chapter summaries</p>
</header>

<nav>
  <a href="index.html">Home</a>
  <a href="topics.html">Topics</a>
  <a href="notes.html">Notes</a>
  <a href="projects.html">Programming</a>
</nav>
<section>
  
  <h2>1. What is a Stochastic Process?</h2>
  <p>
    Derived from the Greek, "Stochastic" means "random" or "chance". It's antonym is "sure", "deterministic", or "certain". While a deterministic model ensures a single outcome from a given set of circumstances, a stochastic model predicts the possible set of outcomes with their respective likelihoods or probabilities.
    While a coin tossed in the air will surely land somewhere on the land around us, whether it lands on its heads or tails is random, with each outcome being assigned a probability of 0.5 .
    It's interesting to note that phenomena are not themselves stochastic or deterministic, it is just how the observer perceives the model. For example, changes in the level of a huge population are often considered and modeled deteministically, although we may agree upon the fact that many chance events contribute to their fluctuations. 
  </p>
  <p>
    As for the definition, <strong>A stochastic process is a family of random variables \( X_t \), where t is a parameter running over a suitable index set T.</strong>
    Usually, t indicates the discrete units of time, and the index set becomes  \( T= \{0,1,2, \ldots \} \). Stochastic processes where \( T= [0,\infty) \) become important in different applications
  </p>
  <h2>2. Review of Probability</h2>
  <h3>
    2.1. Random Variables:
  </h3>
  <p>
    Let \( \Omega \) be a non-empty set equipped with a \( \sigma \)-algebra \( \mathcal{F} \). A random variable is a function \( X : \Omega \to \mathbb{R} \) such that for every Borel set \( A \subseteq \mathbb{R} \), we have the pre-image \( X^{-1}(A) \in \mathcal{F} \); i.e., \(X\) is \(\mathcal{F}\) - measurable.
  </p>
  <h3>
    2.2. Continuous and discrete Random Variables:
  </h3>
  <p>
    <li><strong>CDF:</strong> Let \(X\) be a random vaiable. Then its Cumulative Distribution Function (CDF) is defined as \(F : \mathbb{R} \to [0,1] \) given by $$ F(x)= P(X \leq x), \quad x \in \mathbb{R} $$ </li>
    <li><strong>Continuous Distribution:</strong> A random variable is said to be continuous if its CDF is continuous</li>
    <li><strong>Discrete Distributions:</strong> If the CDF of a random variable has all the properties of a CDF but is discontinuous, it is said to be discrete random variable</li>
    <li><strong>Density function:</strong> A function \(f : \mathbb{R} \to [0, \infty) \) is said to be the density function of some rv having CDF \(F\)  iff $$ \int_{-\infty}^{\infty} f(x)\, dx = 1 $$</li>
    <li><strong>Probability Mass Function (PMF):</strong> A discrete random variable \(X\) taking values \( x_1, x_2, \ldots \) with probabilities \( p_1, p_2, \ldots \),respectively. The Probability Mass function (PMF) of \( X \) is given as \( p : \mathbb{R} \to [0, 1] \), where
\[
p(x) =
  \begin{cases}
    p_1 & \text{if } x = x_1 \\
    p_2 & \text{if } x = x_2 \\
    p_3 & \text{if } x = x_3 \\
    . \\
    . \\
    . \\
    0   & \text{otherwise.}
  \end{cases}
\]
where \( \sum p_i = 1 \).
</li>

  <h3>2.3. Moments and Expectations</h3>
  <p>
    If X is a discrete rv, its \(m\)-th moment is given by $$ E[X^m] = \sum_{i} x_i^m \, \Pr(X = x_i) $$
    For continuous rv, the \(m\)-th moment is given by $$ E[X^m] = \int_{-\infty}^{\infty} x^m \, f(x) dx $$
    The \(m\)-th central moment is defined as the \(m\)-th order moment of the random variable \(X- \mu_x \), provided that \(\mu_x\) exists finitely.
    
  </p>
  <h2>3. Some Discrete Distributions</h2>
  <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; margin: 2em auto; text-align: center;">
  <thead style="background-color: #f0f0f0;">
    <tr>
      <th>Distribution</th>
      <th>Notation</th>
      <th>PMF</th>
      <th>Characteristic Function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Bernoulli</td>
      <td>\( X \sim \mathrm{Bern}(p) \)</td>
      <td>\( p(x) = p^x(1-p)^{1-x},\ x \in \{0,1\} \)</td>
      <td>\( \varphi_X(t) = 1 - p + pe^{it} \)</td>
    </tr>
    <tr>
      <td>Binomial</td>
      <td>\( X \sim \mathrm{Bin}(n, p) \)</td>
      <td>\( p(x) = \binom{n}{x}p^x(1-p)^{n-x} \)</td>
      <td>\( \varphi_X(t) = (1 - p + pe^{it})^n \)</td>
    </tr>
    <tr>
      <td>Poisson</td>
      <td>\( X \sim \mathrm{Pois}(\lambda) \)</td>
      <td>\( p(x) = \frac{e^{-\lambda} \lambda^x}{x!} \)</td>
      <td>\( \varphi_X(t) = \exp(\lambda(e^{it} - 1)) \)</td>
    </tr>
    <tr>
      <td>Geometric</td>
      <td>\( X \sim \mathrm{Geom}(n, p) \)</td>
      <td>\( f(x) = p(1-p)^{x-1}, x \in \{1,2, \ldots\} \)</td>
      <td>\( \varphi_X(t) = \frac{pe^{it}}{1 - (1 - p)e^{it}} \)</td>
    </tr>
  </tbody>
  </table>
  
  <h2>4. Some Continuous Distributions</h2>
  <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; margin: 2em auto; text-align: center;">
  <thead style="background-color: #f0f0f0;">
    <tr>
      <th>Distribution</th>
      <th>Notation</th>
      <th>PDF</th>
      <th>Characteristic Function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Uniform</td>
      <td>\( X \sim \mathrm{Unif}(a,b) \)</td>
      <td>\( f(x) = \frac{1} {b-a},\ x \in [a,b] \)</td>
      <td>\( \varphi_X(t) = \frac{e^{itb}-e^{ita}} {it(b-a)} \)</td>
    </tr>
    <tr>
      <td>Exponential</td>
      <td>\( X \sim \mathrm{Exp}(\lambda) \)</td>
      <td>\( f(x) = \lambda e^{-\lambda x} , \ x \geq 0   \)</td>
      <td>\( \varphi_X(t) = \frac{\lambda}{\lambda -ib} \)</td>
    </tr>
    <tr>
      <td>Gamma</td>
      <td>\( X \sim \mathrm{Gamma}(p (rate), \alpha (shape)) \)</td>
      <td>\( f(x) = \frac{p^{\alpha}x^{\alpha -1}e^{-px}}{\gamma(\alpha)} , \ x > 0 \)</td>
      <td>\( \varphi_X(t) = {(\frac{p}{p-it})}^{\alpha} \)</td>
    </tr>
    <tr>
      <td>Beta</td>
      <td>\( X \sim \mathrm{Beta}(a, b) \)</td>
      <td>\( f(x) = \frac{x^{a-1}(1-x)^{b-1}}{\beta(a,b)}, \ x \in (0,1) \)</td>
      <td>Not elementary in closed form</td>
    </tr>
    <tr>
      <td>Cauchy</td>
      <td>\( X \sim \mathrm{Cauchy}(x_0, \gamma) \)</td>
      <td>\( f(x) = \frac{1}{\pi \gamma \left[1 + \left(\frac{x - x_0}{\gamma}\right)^2 \right]} \)</td>
      <td>\( \varphi_X(t) = e^{i x_0 t - \gamma |t|} \)</td>
    </tr>
    <tr>
      <td>Normal (Univariate)</td>
      <td>\( X \sim \mathcal{N}(\mu, \sigma^2) \)</td>
      <td>\( f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{ -\frac{(x - \mu)^2}{2\sigma^2} } \)</td>
      <td>\( \varphi_X(t) = e^{i \mu t - \frac{1}{2} \sigma^2 t^2} \)</td>
    </tr>
    <tr>
      <td>Normal (Multivariate)</td>
      <td>\( \mathbf{\vec{X}} \sim \mathcal{N}_n(\boldsymbol{\vec{\mu}}, \boldsymbol{\Sigma}) \)</td>
      <td>\( f(\mathbf{\vec{x}}) = \frac{1}{(2\pi)^{n/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{\vec{x}} - \boldsymbol{\vec{\mu}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{\vec{x}} - \boldsymbol{\vec{\mu}}) \right) \)</td>
      <td>\( \varphi_{\mathbf{X}}(\mathbf{t}) = \exp\left( i \mathbf{t}^\top \boldsymbol{\mu} - \frac{1}{2} \mathbf{t}^\top \boldsymbol{\Sigma} \mathbf{t} \right) \)</td>
    </tr>
    <tr>
      <td>Chi-square</td>
      <td>\( X \sim \chi^2_k \)</td>
      <td>\( f(x) = \frac{1}{2^{k/2} \Gamma(k/2)} x^{k/2 - 1} e^{-x/2},\ x > 0 \)</td>
      <td>\( \varphi_X(t) = (1 - 2it)^{-k/2} \)</td>
    </tr>
    <tr>
      <td>t-distribution</td>
      <td>\( T \sim t_\nu \)</td>
      <td>\( f(t) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu \pi} \ \Gamma\left(\frac{\nu}{2}\right)} \left(1 + \frac{t^2}{\nu}\right)^{-(\nu + 1)/2} \)</td>
      <td>No simple closed form, but the k-th order moment exists only when the degrees of freedom (= n) >k</td>
    </tr>
    <tr>
      <td>F-distribution</td>
      <td>\( F \sim F_{d_1, d_2} \)</td>
      <td>\( f(x) = \frac{1}{B\left(\frac{d_1}{2}, \frac{d_2}{2}\right)} \left( \frac{d_1}{d_2} \right)^{d_1/2} \frac{x^{d_1/2 - 1}}{\left(1 + \frac{d_1}{d_2} x \right)^{(d_1 + d_2)/2}},\ x > 0 \)</td>
      <td>No simple closed form, but the k-th moment exists only if \(d_2\) > 2ùëü </td>
    </tr>
  </tbody>
  </table>
  <h2>5. Inequalities at a glance</h2>
  <h3>5.1 The Schwarz Inequality</h3>
  <p>
    Let X and Y have finite second moments. Then <strong>$$ {E[XY]}^2 \leq E[X^2]E[Y^2] $$</strong>
  </p>
  <h3>5.2 The Markov Inequality</h3>
  <p>
    Let X be a non-negative random variable, and t be a positive number. Then <strong>$$ P(X \geq t) \leq \frac{E[X]}{t} $$</strong>
  </p>
  <h3>5.3 The Chebyshev's Inequality</h3>
  <p>
    Let X be a random variable with mean \( \mu \) and finite variance \( \sigma^2 \) . Then for any real number t > 0  <strong>$$ P(|X- \mu| \geq t) \leq \frac{\sigma^2}{t^2} $$</strong>
  </p>
                                                                                                                                                                             
  <h2>6. Random Sums</h2>
  <p>
    Let \( X_1 , X_2, \ldots \) be a sequence of IID random variables, and let N be a discrete random variable, independent of all \(X_i\)s, and having PMF \( p_N (n) = \Pr (N=n), \ n \in {0,1,2, \ldots } \) We define the random sum \(S_N\) as $$ S_N = \sum_{i=1}^{N} X_i, \ N>0 $$
  </p>
  <h3>Examples</h3>
  <p>
    (a) Queueing: N= number of customers arriving at a service facility in a given time period, \(X_i\) = Service time for the i-th customer. Then \(S_N\) = Total demand for service time. 
  </p>
  <p>
    (b) Risk Theory: N= total number of claims arriving at insurance company, \(X_i\)= amount of the i-th claim. Then \(S_N\) = Total liability of the insurance company.
  </p>
  <h2>7. Moments of a Random Sum</h2>
  <p>
    Let's assume that \(E[X_i] = \mu, E[N] = \nu, Var[X_i] = \sigma^2 , Var[N] = \tau^2. \)
  </p>
  <p>
    We now find the value of \(E[S_N]\). 
    This can be found out as \(E[S_N] \)
    $$= \sum_{n=0}^{\infty} E[S_N|N=n]p_N(n) $$
    $$= \sum_{n=1}^{\infty} E[X_1 + X_2 + X_3 + \ldots + X_N |N=n]p_N(n) $$
    $$= \sum_{n=1}^{\infty} E[X_1 + X_2 + X_3 + \ldots + X_n]p_N(n) $$
    $$= \sum_{n=1}^{\infty} \mu np_N(n) $$
    $$= \mu \sum_{n=1}^{\infty} np_N(n) $$
    $$= \mu \nu $$
  </p>
  <p>
    We can as well find \( Var[S_N] \) which turns out to be $$ Var[S_N] = \mu^2 \tau^2 + \nu\sigma^2 $$
  </p>
  <h3>8. The Distribution of a Random Sum</h3>
  Consider the previous setup of sum of Random Variables. We are now interested in finding the distribution of their sum, given each of them have the density \(f(x)\). Then we know, 
  $$ f_{S_1}(x) = f_{X_1}(x) = f(x) $$
  $$ f_{S_2}(x) = f_{X_1 + X_2}(x) $$
  which can be found using the transformation \(h(Y_1,Y_2)= (X_1,X_1+X_2) \)
  this turns out to be 
  $$ f_{S_2}(x) = \int_{\infty}^{\infty} f(z)f(x-z) dz $$
  Similarly, 
  $$ f_{S_n}(x) = \int_{\infty}^{\infty} f_{S_{n-1}}(x-z)f(z) dz $$
  <h4>8.1 On Stock Price Changes:</h4>
  <p>
    Let Z = Closing price on the (n+1)th day - Closing price on the nth day
    Then Z = \(\sum_{i=1}^{N} X_i\), where \(X_i\) denotes the i-th transaction, each of which are independent , with a finite variance.
    As a result, CLT applies, suggesting it should be approximately normally distributed. This is the classical assumption used in many financial models. However, In real data, very small and very large changes happen more often than a normal distribution predicts, while medium-sized changes occur less often than expected.
    This might be because Z is rather a random sum, where N is random.
    Let's assume \( N \sim \mathcal{Pois}(\lambda)\) and \( X_i \sim \mathbb{N}(0, \sigma^2) \) 
    Then \( Z|n \sim \mathbb{N}(0, (n+1)\sigma^2)\) & \( p_N (n) = \frac{\lambda^n e^{-\lambda}}{n!} \)
    Using this two equations, we get: 
    $$ f_Z(z) = \sum_{n=0}^{\infty} \frac{exp(\frac{z^2}{2(n+1)\sigma^2})}{\sqrt{2\pi(n+1)}\sigma} \frac{\lambda^n e^{-\lambda}}{n!}$$
    The resulting distribution is not normal but still has mean 0 and finite variance. This model explains why we might see heavy tails (more extreme events) and sharper peaks in the distribution of stock price changes ‚Äî without violating the independence or Gaussian assumptions per transaction. The only difference is adding randomness to how many such events happen per day.
    This random-sum model produces a distribution that resembles real-world data better than the simple normal model. It's not a "proof" that transaction count randomness causes this ‚Äî just that it‚Äôs a plausible explanation supported by the math
  </p>
  <figure style="text-align: center;">
    <img src="tk-intro1.png" alt="Stock Price Model" width="500">
    <figcaption><em>Figure 1:</em> The dashed line (random sum model) is more peaked and heavier-tailed than the solid normal curve ‚Äî matching real stock data more closely.</figcaption>
  </figure>
  <h3> 9. MARTINGALES</h3>
  <h4>9.1 Definition</h4>
  <p>
    A stochastic process \(X_n : n= 0,1, \ldots \) is said to be martingale if \(&forall; n= 0,1, \ldots\) 
    $$ (a) E[|X_n|] < \infty $$
    $$ (b) E[X_{n+1}| X_0, X_1, \ldots , X_n ] = X_n $$
  </p>
  <p>
    for the (b) part, we observe that $$ E[E[X_{n+1}| X_0, X_1, \ldots , X_n ]] = E[X_n] $$
    also by the tower property, $$ E[E[X_{n+1}| X_0, X_1, \ldots , X_n ]] = E[X_{n+1}] $$
    thus, we can observe that \(E[X_0]= E[X_k] &forall; k =0,1, \ldots \)  
  </p>
  <h4>9.2 Martingales and Fairness in Gambling & Markets</h4>
  <p>
    The martingale property captures the idea of fairness in gambling: a player‚Äôs expected fortune after the next play equals the current fortune, regardless of past outcomes. This reflects a fair game, where no betting strategy (like the classic "martingale" doubling approach) can guarantee profit in the long run. While martingale theory has roots in gambling, it now finds broad applications beyond it.
  </p>
  <p>
    In financial markets, martingales model the notion of efficient pricing. For a security like a stock, if future prices could be predicted, buying or selling pressure would shift the current price. In an ideal, perfect market, such adjustments lead to a situation where future prices are, on average, unpredictable, making the price process a martingale.
  </p>
  <h4>9.3 Martingales and Martingale Difference Sequences</h4>
  <p>
    The most basic examples of martingales are sums of independent, mean zero random variables.
    Let \( Y_0, Y_1, \ldots \) be a sequence of independent, identically distributed random variables such that 
    \( E[Y_n] = 0 \). Then the sequence of partial sums
  </p>
  <p>
    \[ X_n = \sum_{j=1}^{n} Y_j \]
    is a martingale relative to the sequence \( \{0, Y_1, Y_2, \ldots\} \) (that is, relative to the natural filtration
    generated by the variables \( Y_n \)). This is easily verified using the linearity and stability properties
    and the independence law for conditional expectation:
  </p>
  <p>
    \[
    \begin{align*}
    E[X_{n+1} \mid \mathcal{F}_n] &= E[X_n + Y_{n+1} \mid \mathcal{F}_n] \\ 
    &= E[X_n \mid \mathcal{F}_n] + E[Y_{n+1} \mid \mathcal{F}_n] \\ 
    &= X_n + 0 = X_n
    \end{align*}
    \]
  </p>
  <p>
    The importance of martingales in modern probability stems at least in part from the fact that most of the essential 
    properties of sums of independent, identically distributed random variables are inherited (with minor modification) by martingales.
  </p>
  <h5>Proposition 1</h5>
  <p>
    The martingale difference sequence \( \{\varepsilon_n\} \) has the following properties:
    <br>(a) The random variable \( \varepsilon_n \) is a function of \( \mathcal{F}_n \);
    <br>(b) For every \( n \geq 0 \), \( E[\varepsilon_{n+1} \mid \mathcal{F}_n] = 0 \).
  </p>
  <p>
    <strong>Proof:</strong>Assertion (b) is a three-line calculation using the properties of conditional expectation
    and the definition of a martingale.
  </p>
  <h5>Corollary 1</h5>
  <p>
    Let \( \{X_n\} \) be a martingale relative to \( \{Y_n\} \), with martingale difference sequence \( \{\varepsilon_n\} \).
    Then for every \( n \geq 0 \),
    <br>\( E[X_n] = E[X_0] \).
    <br>Moreover, if \( X_0 = 0 \) and \( E[X_n^2] < \infty \) then the random variables \( \varepsilon_j \) are uncorrelated, and so
  </p>

  <p>
    \[
    E[X_n^2] = \sum_{j=1}^{n} E[\varepsilon_j^2]
    \]
  </p>
  <p>
    <strong>Proof:</strong>The first property follows almost trivially from Proposition 1 and the Expectation Law for conditional expectation,
    as these together imply that \( E[\varepsilon_n] = 0 \) for each \( n \). Summing and using linearity of expectation gives the result.
  </p>

  <p>
    The second property follows by observing that each \( \varepsilon_j \) has finite variance (since it is the difference
    of two variables with finite second moments). Using Cauchy-Schwarz, we can ensure the cross-products have finite first moments.
    Then, for \( j < k \leq n \), \( \varepsilon_j \) is \( \mathcal{F}_j \)-measurable, and by the properties of conditional expectation,
    \( E[\varepsilon_j \varepsilon_k] = E[\varepsilon_j] E[\varepsilon_k] = 0 \).
  </p>

  <p>
    Hence the variance of \( X_n \) is the sum of variances of the \( \varepsilon_j \), analogous to the case of i.i.d. mean-zero sums.
  </p>
  <h4>9.5 Maximal Inequality for Non-negative martingales</h4>
  <p>
    Let \( \{X_n\}_{n \geq 0} \) be a non-negative submartingale. Then for any \( \lambda > 0 \),
  </p>
  <p style="text-align: center; font-weight: bold;">
    \[
    \mathbb{P}\left( \max_{0 \leq k \leq n} X_k \geq \lambda \right) \leq \frac{\mathbb{E}[X_n]}{\lambda}
    \]
  </p>
  <p><strong>Proof:</strong></p>
  <p>Let \( X_0, X_1, \ldots, X_n \) be non-negative random variables such that:</p>
  <ul>
    <li>\( \mathbb{E}[X_k] < \infty \) for all \( k \),</li>
    <li>\( \mathbb{E}[X_k] \le \mathbb{E}[X_{k+1}] \) (non-decreasing in expectation).</li>
  </ul>
  <p>We want to prove the inequality:</p>

  <div class="proof-box">
    <p>
      \[
      \mathbb{P}\left(\max_{0 \le k \le n} X_k \ge \lambda\right) \le \frac{\mathbb{E}[X_n]}{\lambda}, \quad \text{for all } \lambda > 0.
      \]
    </p>
  </div>
  <p>Define the event:</p>
  <p>\( A := \left\{ \max_{0 \le k \le n} X_k \ge \lambda \right\} \).</p>

  <p>For each \( k \), define the indicator:</p>

  <p>
    \[
    I_k := 
    \begin{cases}
    1, & \text{if } X_k \ge \lambda \text{ and } X_j < \lambda \text{ for all } j < k, \\
    0, & \text{otherwise}.
    \end{cases}
    \]
  </p>

  <p>Then exactly one \( I_k = 1 \) on the event \( A \), so \( \sum_{k=0}^n I_k = \mathbf{1}_A \).</p>

  <p>Now observe that:</p>

  <p>
    \[
    X_n \ge \sum_{k=0}^{n} X_k I_k \ge \sum_{k=0}^{n} \lambda I_k = \lambda \cdot \mathbf{1}_A.
    \]
  </p>

  <p>Taking expectation:</p>

  <p>
    \[
    \mathbb{E}[X_n] \ge \mathbb{E}\left[ \lambda \cdot \mathbf{1}_A \right] = \lambda \cdot \mathbb{P}(A).
    \]
  </p>

  <p>Rearranging gives the desired inequality:</p>

  <div class="proof-box">
    <p>
      \[
      \mathbb{P}\left( \max_{0 \le k \le n} X_k \ge \lambda \right) \le \frac{\mathbb{E}[X_n]}{\lambda}.
      \]
    </p>
  </div>
  <h4>9.6 Urn Game and Martingale</h4>
  <h5>Intuitive Explanation</h5>
  <p>We start with an urn containing <strong>1 red</strong> and <strong>1 green</strong> ball. We play the following game repeatedly:</p>
    <ul>
        <li>Draw one ball at random.</li>
        <li>Return it to the urn along with another ball of the <strong>same color</strong>.</li>
    </ul>
    <p>This process continues indefinitely. Let \(X_n\) be the fraction of red balls after the \(n\)th draw.</p>
    <p>At each step, the probability of drawing a red ball is \(X_n\), and green is \(1 - X_n\). The key point is:</p>
    <blockquote><em>The expected value of \(X_{n+1}\) given \(X_n\) is exactly \(X_n\). This makes \(X_n\) a martingale.</em></blockquote>
    <p>Although every player's sequence of red-ball fractions will stabilize to a fixed number, that number is <strong>random</strong> and varies across different games. In fact, the final limiting value is <strong>uniformly distributed</strong> on the interval (0, 1).</p>

    <h5>Analytical Proof</h5>
    <p>Let \(R_n\) be the number of red balls after the \(n\)th draw. Then the total number of balls is \(n + 2\). The fraction of red balls is:</p>
    \(X_n = R_n / (n + 2)\)

    <p>At step \(n+1\):</p>
    <ul>
        <li>With probability \(X_n\), a red ball is drawn: \(R_{n+1} = R_n + 1\)</li>
        <li>With probability \(1 - X_n\), a green ball is drawn: \(R_{n+1} = R_n\)</li>
    </ul>

    <p>So the expected number of red balls after the next draw is:</p>
    \(E[R_{n+1} | X_n] = R_n + X_n\)

    <p>Then:</p>
    \(
E[X_{n+1} | X_n] = E[R_{n+1} / (n+3) | X_n] 
                = (R_n + X_n) / (n + 3) \)
                <p> \(= ((n + 2)X_n + X_n) / (n + 3) 
                = X_n\) </p>

    <p>This confirms that \(X_n\) is a <strong>martingale</strong>.</p>

    <h5>Limit Distribution</h5>
    <p>Let us compute the probability distribution of \(R_n\). By iteratively applying the rules, we find:</p>
    \(P(R_n = k) = 1 / (n + 1), &forall; k = 1, 2, ..., n + 1\)

    <p>Therefore, the values of \(X_n = k / (n + 2)\) are <strong>equally likely</strong>. In the limit as \(n &rarr; &infin;\), we get:</p>
    \(P(X = x) = x, &forall; 0 &lt; x &lt; 1\)

    <p>So the limit \(X\) is uniformly distributed over \((0, 1)\).</p>

    <h5>Conclusion</h5>
    <p>This example shows how a simple probabilistic process can result in a complex and interesting distribution. It also illustrates the power of martingales in analyzing such processes, even though the randomness seems to grow over time.</p>
<h4>10. Few Problems for revision:</h4>
<h5>Question 1:</h5>

<button class="hint-button" onclick="toggleHint()">Show/Hide Solution</button>
<div class="hint-text" id="hint">
  <h5>Solution</h5>
  <div class="solution">
  </div>
</div>






      
<h5>Question 2:</h5>
<p>Consider a stochastic process that evolves according to the following rules:</p>
    <ul>
      <li>If <code>\(X_0\) = 0</code>, then <code>\(X_n\) = 0</code> for all <code>n ‚â• 0</code>.</li>
      <li>If <code>\(X_n\) > 0</code>, then
        <ul>
          <li><code>\(X_{n+1}\) = \(X_n\) + 1</code> with probability 1/2,</li>
          <li><code>\(X_{n+1}\) = \(X_n\) - 1</code> with probability 1/2.</li>
        </ul>
      </li>
    </ul>
    <p>
      (a) Show that \(X_n\) is a nonnegative martingale.
    </p>
    <p>
      (b) Suppose that <code>\(X_0\) = i &gt; 0</code>. Use the maximal inequality to bound
      <code>Pr(\(X_n\) ‚â• M for some n ‚â§ N)</code>. Note: \(X_n\) represents the fortune of a player in a fair game who wagers $1 at each bet and quits if all money is lost (<code>X = 0</code>).
    </p>
<button class="hint-button" onclick="toggleHint()">Show/Hide Solution</button>
<div class="hint-text" id="hint">
  <h5>Solution</h5>
  <div class="solution">
    <h6>(a) Show that <code>\(X_n\)</code> is a nonnegative martingale</h6>
    <p>We check two conditions for a martingale:</p>
    <ul>
      <li><strong>Integrability:</strong> Since the values of <code>\(X_n\)</code> are bounded below by 0 and change by ¬±1 at each step, <code>E[|\(X_n\)|] &lt; \(\infty\)</code>.</li>
      <li><strong>Martingale Property:</strong>
        <ul>
          <li>If <code>\(X_n\) = 0</code>, then <code>\(X_{n+1}\) = 0</code> with probability 1, so <code>E[\(X_{n+1}\) | \(X_n\)] = 0 = \(X_n\)</code>.</li>
          <li>If <code>\(X_n\) > 0</code>, then:
            <p><code>
              E[\(X_{n+1}\) | \(X_n\)] = (1/2)(\(X_n\) + 1) + (1/2)(\(X_n\) - 1) = \(X_n\)
            </code></p>
          </li>
        </ul>
      </li>
    </ul>
    <p>Thus, <code>\(X_n\)</code> is a nonnegative martingale.</p>

    <h6>(b) Maximal Inequality Bound</h6>
    <p>We use the maximal inequality for nonnegative martingales:</p>
    <p><code>
      Pr(\(max_o\) ‚â§ k ‚â§ N \(X_k\) ‚â• M) ‚â§ E[\(X_N\)] / M
    </code></p>
    <p>Since <code>X‚Çô</code> is a martingale with <code>X‚ÇÄ = i</code>, we have <code>E[X_N] = i</code>. Therefore:</p>
    <p><code>
      Pr(\(max_o\) ‚â§ k ‚â§ N \(X_k\) ‚â• M) ‚â§ i / M
    </code></p>
    <p>Example: If <code>M = 2i</code>, then the probability the gambler ever reaches twice their fortune is ‚â§ 1/2.</p>
  </div>
</div>


</section>

<div class="nav-bottom">
  <a href="ch0-preface.html">&larr; Previous Chapter</a>
  <a href="tk-markov-chains1.html">Next Chapter &rarr;</a>
</div>

<footer>
  &copy; 2025 Mohammad Shaan | This site is maintained on GitHub Pages.
</footer>
<!-- MathJax for LaTeX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  function toggleHint() {
    var hint = document.getElementById("hint");
    if (hint.style.display === "none" || hint.style.display === "") {
      hint.style.display = "block";
    } else {
      hint.style.display = "none";
    }
  }
</script>

</body>
</html>
