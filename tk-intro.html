<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Stochastic Processes & Finance ‚Äì Summer Reading Project</title>
  <link href="https://fonts.googleapis.com/css2?family=Lato&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Lato', sans-serif;
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      color: #333;
    }
    header {
      background-color: #003366;
      color: white;
      padding: 1em;
      text-align: center;
    }
    nav {
      background-color: #e0e0e0;
      padding: 1em;
      text-align: center;
    }
    nav a {
      margin: 0 1em;
      text-decoration: none;
      color: #003366;
      font-weight: bold;
    }
    section {
      max-width: 1200px;
      margin: 2em auto;
      padding: 1em;
      background: white;
      border-radius: 8px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }

    .nav-bottom a {
      flex: 1;
      text-align: center;
      padding: 0.5em 1em;
      color: #003366;
      font-weight: bold;
      text-decoration: none;
    }

    footer {
      text-align: center;
      font-size: 0.9em;
      color: #777;
      margin: 2em 0;
    }
    .hint-text {
      display: none;
      margin-top: 0.5em;
      background: #f0f0f0;
      padding: 0.5em;
      border-left: 4px solid #0077cc;
      border-radius: 4px;
    }

    .hint-button {
      margin-top: 1em;
      background-color: #0077cc;
      color: white;
      border: none;
      padding: 0.4em 0.8em;
      border-radius: 4px;
      cursor: pointer;
      font-size: 0.9em;
    }

    .hint-button:hover {
      background-color: #005fa3;
    }
  </style>
</head>
<body>

<header>
  <h1>Chapter 1: Introduction to Stochastic Processes</h1>
  <p>Study notes and chapter summaries</p>
</header>

<nav>
  <a href="index.html">Home</a>
  <a href="topics.html">Topics</a>
  <a href="notes.html">Notes</a>
  <a href="projects.html">Programming</a>
</nav>
<section>
  
  <h2>1. What is a Stochastic Process?</h2>
  <p>
    Derived from the Greek, "Stochastic" means "random" or "chance". It's antonym is "sure", "deterministic", or "certain". While a deterministic model ensures a single outcome from a given set of circumstances, a stochastic model predicts the possible set of outcomes with their respective likelihoods or probabilities.
    While a coin tossed in the air will surely land somewhere on the land around us, whether it lands on its heads or tails is random, with each outcome being assigned a probability of 0.5 .
    It's interesting to note that phenomena are not themselves stochastic or deterministic, it is just how the observer perceives the model. For example, changes in the level of a huge population are often considered and modeled deteministically, although we may agree upon the fact that many chance events contribute to their fluctuations. 
  </p>
  <p>
    As for the definition, <strong>A stochastic process is a family of random variables \( X_t \), where t is a parameter running over a suitable index set T.</strong>
    Usually, t indicates the discrete units of time, and the index set becomes  \( T= \{0,1,2, \ldots \} \). Stochastic processes where \( T= [0,\infty) \) become important in different applications
  </p>
  <h2>2. Review of Probability</h2>
  <h3>
    2.1. Random Variables:
  </h3>
  <p>
    Let \( \Omega \) be a non-empty set equipped with a \( \sigma \)-algebra \( \mathcal{F} \). A random variable is a function \( X : \Omega \to \mathbb{R} \) such that for every Borel set \( A \subseteq \mathbb{R} \), we have the pre-image \( X^{-1}(A) \in \mathcal{F} \); i.e., \(X\) is \(\mathcal{F}\) - measurable.
  </p>
  <h3>
    2.2. Continuous and discrete Random Variables:
  </h3>
  <p>
    <li><strong>CDF:</strong> Let \(X\) be a random vaiable. Then its Cumulative Distribution Function (CDF) is defined as \(F : \mathbb{R} \to [0,1] \) given by $$ F(x)= P(X \leq x), \quad x \in \mathbb{R} $$ </li>
    <li><strong>Continuous Distribution:</strong> A random variable is said to be continuous if its CDF is continuous</li>
    <li><strong>Discrete Distributions:</strong> If the CDF of a random variable has all the properties of a CDF but is discontinuous, it is said to be discrete random variable</li>
    <li><strong>Density function:</strong> A function \(f : \mathbb{R} \to [0, \infty) \) is said to be the density function of some rv having CDF \(F\)  iff $$ \int_{-\infty}^{\infty} f(x)\, dx = 1 $$</li>
    <li><strong>Probability Mass Function (PMF):</strong> A discrete random variable \(X\) taking values \( x_1, x_2, \ldots \) with probabilities \( p_1, p_2, \ldots \),respectively. The Probability Mass function (PMF) of \( X \) is given as \( p : \mathbb{R} \to [0, 1] \), where
\[
p(x) =
  \begin{cases}
    p_1 & \text{if } x = x_1 \\
    p_2 & \text{if } x = x_2 \\
    p_3 & \text{if } x = x_3 \\
    . \\
    . \\
    . \\
    0   & \text{otherwise.}
  \end{cases}
\]
where \( \sum p_i = 1 \).
</li>

  <h3>2.3. Moments and Expectations</h3>
  <p>
    If X is a discrete rv, its \(m\)-th moment is given by $$ E[X^m] = \sum_{i} x_i^m \, \Pr(X = x_i) $$
    For continuous rv, the \(m\)-th moment is given by $$ E[X^m] = \int_{-\infty}^{\infty} x^m \, f(x) dx $$
    The \(m\)-th central moment is defined as the \(m\)-th order moment of the random variable \(X- \mu_x \), provided that \(\mu_x\) exists finitely.
    
  </p>
  <h2>3. Some Discrete Distributions</h2>
  <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; margin: 2em auto; text-align: center;">
  <thead style="background-color: #f0f0f0;">
    <tr>
      <th>Distribution</th>
      <th>Notation</th>
      <th>PMF</th>
      <th>Characteristic Function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Bernoulli</td>
      <td>\( X \sim \mathrm{Bern}(p) \)</td>
      <td>\( p(x) = p^x(1-p)^{1-x},\ x \in \{0,1\} \)</td>
      <td>\( \varphi_X(t) = 1 - p + pe^{it} \)</td>
    </tr>
    <tr>
      <td>Binomial</td>
      <td>\( X \sim \mathrm{Bin}(n, p) \)</td>
      <td>\( p(x) = \binom{n}{x}p^x(1-p)^{n-x} \)</td>
      <td>\( \varphi_X(t) = (1 - p + pe^{it})^n \)</td>
    </tr>
    <tr>
      <td>Poisson</td>
      <td>\( X \sim \mathrm{Pois}(\lambda) \)</td>
      <td>\( p(x) = \frac{e^{-\lambda} \lambda^x}{x!} \)</td>
      <td>\( \varphi_X(t) = \exp(\lambda(e^{it} - 1)) \)</td>
    </tr>
    <tr>
      <td>Geometric</td>
      <td>\( X \sim \mathrm{Geom}(n, p) \)</td>
      <td>\( f(x) = p(1-p)^{x-1}, x \in \{1,2, \ldots\} \)</td>
      <td>\( \varphi_X(t) = \frac{pe^{it}}{1 - (1 - p)e^{it}} \)</td>
    </tr>
  </tbody>
  </table>
  
  <h2>4. Some Continuous Distributions</h2>
  <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; margin: 2em auto; text-align: center;">
  <thead style="background-color: #f0f0f0;">
    <tr>
      <th>Distribution</th>
      <th>Notation</th>
      <th>PDF</th>
      <th>Characteristic Function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Uniform</td>
      <td>\( X \sim \mathrm{Unif}(a,b) \)</td>
      <td>\( f(x) = \frac{1} {b-a},\ x \in [a,b] \)</td>
      <td>\( \varphi_X(t) = \frac{e^{itb}-e^{ita}} {it(b-a)} \)</td>
    </tr>
    <tr>
      <td>Exponential</td>
      <td>\( X \sim \mathrm{Exp}(\lambda) \)</td>
      <td>\( f(x) = \lambda e^{-\lambda x} , \ x \geq 0   \)</td>
      <td>\( \varphi_X(t) = \frac{\lambda}{\lambda -ib} \)</td>
    </tr>
    <tr>
      <td>Gamma</td>
      <td>\( X \sim \mathrm{Gamma}(p (rate), \alpha (shape)) \)</td>
      <td>\( f(x) = \frac{p^{\alpha}x^{\alpha -1}e^{-px}}{\gamma(\alpha)} , \ x > 0 \)</td>
      <td>\( \varphi_X(t) = {(\frac{p}{p-it})}^{\alpha} \)</td>
    </tr>
    <tr>
      <td>Beta</td>
      <td>\( X \sim \mathrm{Beta}(a, b) \)</td>
      <td>\( f(x) = \frac{x^{a-1}(1-x)^{b-1}}{\beta(a,b)}, \ x \in (0,1) \)</td>
      <td>Not elementary in closed form</td>
    </tr>
    <tr>
      <td>Cauchy</td>
      <td>\( X \sim \mathrm{Cauchy}(x_0, \gamma) \)</td>
      <td>\( f(x) = \frac{1}{\pi \gamma \left[1 + \left(\frac{x - x_0}{\gamma}\right)^2 \right]} \)</td>
      <td>\( \varphi_X(t) = e^{i x_0 t - \gamma |t|} \)</td>
    </tr>
    <tr>
      <td>Normal (Univariate)</td>
      <td>\( X \sim \mathcal{N}(\mu, \sigma^2) \)</td>
      <td>\( f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{ -\frac{(x - \mu)^2}{2\sigma^2} } \)</td>
      <td>\( \varphi_X(t) = e^{i \mu t - \frac{1}{2} \sigma^2 t^2} \)</td>
    </tr>
    <tr>
      <td>Normal (Multivariate)</td>
      <td>\( \mathbf{\vec{X}} \sim \mathcal{N}_n(\boldsymbol{\vec{\mu}}, \boldsymbol{\Sigma}) \)</td>
      <td>\( f(\mathbf{\vec{x}}) = \frac{1}{(2\pi)^{n/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{\vec{x}} - \boldsymbol{\vec{\mu}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{\vec{x}} - \boldsymbol{\vec{\mu}}) \right) \)</td>
      <td>\( \varphi_{\mathbf{X}}(\mathbf{t}) = \exp\left( i \mathbf{t}^\top \boldsymbol{\mu} - \frac{1}{2} \mathbf{t}^\top \boldsymbol{\Sigma} \mathbf{t} \right) \)</td>
    </tr>
    <tr>
      <td>Chi-square</td>
      <td>\( X \sim \chi^2_k \)</td>
      <td>\( f(x) = \frac{1}{2^{k/2} \Gamma(k/2)} x^{k/2 - 1} e^{-x/2},\ x > 0 \)</td>
      <td>\( \varphi_X(t) = (1 - 2it)^{-k/2} \)</td>
    </tr>
    <tr>
      <td>t-distribution</td>
      <td>\( T \sim t_\nu \)</td>
      <td>\( f(t) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu \pi} \ \Gamma\left(\frac{\nu}{2}\right)} \left(1 + \frac{t^2}{\nu}\right)^{-(\nu + 1)/2} \)</td>
      <td>No simple closed form, but the k-th order moment exists only when the degrees of freedom (= n) >k</td>
    </tr>
    <tr>
      <td>F-distribution</td>
      <td>\( F \sim F_{d_1, d_2} \)</td>
      <td>\( f(x) = \frac{1}{B\left(\frac{d_1}{2}, \frac{d_2}{2}\right)} \left( \frac{d_1}{d_2} \right)^{d_1/2} \frac{x^{d_1/2 - 1}}{\left(1 + \frac{d_1}{d_2} x \right)^{(d_1 + d_2)/2}},\ x > 0 \)</td>
      <td>No simple closed form, but the k-th moment exists only if \(d_2\) > 2ùëü </td>
    </tr>
  </tbody>
  </table>
  <h2>5. Inequalities at a glance</h2>
  <h3>5.1 The Schwarz Inequality</h3>
  <p>
    Let X and Y have finite second moments. Then <strong>$$ {E[XY]}^2 \leq E[X^2]E[Y^2] $$</strong>
  </p>
  <h3>5.2 The Markov Inequality</h3>
  <p>
    Let X be a non-negative random variable, and t be a positive number. Then <strong>$$ P(X \geq t) \leq \frac{E[X]}{t} $$</strong>
  </p>
  <h3>5.3 The Chebyshev's Inequality</h3>
  <p>
    Let X be a random variable with mean \( \mu \) and finite variance \( \sigma^2 \) . Then for any real number t > 0  <strong>$$ P(|X- \mu| \geq t) \leq \frac{\sigma^2}{t^2} $$</strong>
  </p>
                                                                                                                                                                             
  <h2>6. Random Sums</h2>
  <p>
    Let \( X_1 , X_2, \ldots \) be a sequence of IID random variables, and let N be a discrete random variable, independent of all \(X_i\)s, and having PMF \( p_N (n) = \Pr (N=n), \ n \in {0,1,2, \ldots } \) We define the random sum \(S_N\) as $$ S_N = \sum_{i=1}^{N} X_i, \ N>0 $$
  </p>
  <h3>Examples</h3>
  <p>
    (a) Queueing: N= number of customers arriving at a service facility in a given time period, \(X_i\) = Service time for the i-th customer. Then \(S_N\) = Total demand for service time. 
  </p>
  <p>
    (b) Risk Theory: N= total number of claims arriving at insurance company, \(X_i\)= amount of the i-th claim. Then \(S_N\) = Total liability of the insurance company.
  </p>
  <h2>7. Moments of a Random Sum</h2>
  <p>
    Let's assume that \(E[X_i] = \mu, E[N] = \nu, Var[X_i] = \sigma^2 , Var[N] = \tau^2. \)
  </p>
  <p>
    We now find the value of \(E[S_N]\). 
    This can be found out as \(E[S_N] \)
    $$= \sum_{n=0}^{\infty} E[S_N|N=n]p_N(n) $$
    $$= \sum_{n=1}^{\infty} E[X_1 + X_2 + X_3 + \ldots + X_N |N=n]p_N(n) $$
    $$= \sum_{n=1}^{\infty} E[X_1 + X_2 + X_3 + \ldots + X_n]p_N(n) $$
    $$= \sum_{n=1}^{\infty} \mu np_N(n) $$
    $$= \mu \sum_{n=1}^{\infty} np_N(n) $$
    $$= \mu \nu $$
  </p>
  <p>
    We can as well find \( Var[S_N] \) which turns out to be $$ Var[S_N] = \mu^2 \tau^2 + \nu\sigma^2 $$
  </p>
  <h3>8. The Distribution of a Random Sum</h3>
  Consider the previous setup of sum of Random Variables. We are now interested in finding the distribution of their sum, given each of them have the density \(f(x)\). Then we know, 
  $$ f_{S_1}(x) = f_{X_1}(x) = f(x) $$
  $$ f_{S_2}(x) = f_{X_1 + X_2}(x) $$
  which can be found using the transformation \(h(Y_1,Y_2)= (X_1,X_1+X_2) \)
  this turns out to be 
  $$ f_{S_2}(x) = \int_{\infty}^{\infty} f(z)f(x-z) dz $$
  Similarly, 
  $$ f_{S_n}(x) = \int_{\infty}^{\infty} f_{S_{n-1}}(x-z)f(z) dz $$
  <h4>8.1 On Stock Price Changes:</h4>
  <p>
    Let Z = Closing price on the (n+1)th day - Closing price on the nth day
    Then Z = \(\sum_{i=1}^{N} X_i\), where \(X_i\) denotes the i-th transaction, each of which are independent , with a finite variance.
    As a result, CLT applies, suggesting it should be approximately normally distributed. This is the classical assumption used in many financial models. However, In real data, very small and very large changes happen more often than a normal distribution predicts, while medium-sized changes occur less often than expected.
    This might be because Z is rather a random sum, where N is random.
    Let's assume \( N \sim \mathcal{Pois}(\lambda)\) and \( X_i \sim \mathbb{N}(0, \sigma^2) \) 
    Then \( Z|n \sim \mathbb{N}(0, (n+1)\sigma^2)\) & \( p_N (n) = \frac{\lambda^n e^{-\lambda}}{n!} \)
    Using this two equations, we get: 
    $$ f_Z(z) = \sum_{n=0}^{\infty} \frac{exp(\frac{z^2}{2(n+1)\sigma^2})}{\sqrt{2\pi(n+1)}\sigma} \frac{\lambda^n e^{-\lambda}}{n!}$$
    The resulting distribution is not normal but still has mean 0 and finite variance. This model explains why we might see heavy tails (more extreme events) and sharper peaks in the distribution of stock price changes ‚Äî without violating the independence or Gaussian assumptions per transaction. The only difference is adding randomness to how many such events happen per day.
    This random-sum model produces a distribution that resembles real-world data better than the simple normal model. It's not a "proof" that transaction count randomness causes this ‚Äî just that it‚Äôs a plausible explanation supported by the math
  </p>
  <figure style="text-align: center;">
    <img src="tk-intro1.png" alt="Stock Price Model" width="500">
    <figcaption><em>Figure 1:</em> The dashed line (random sum model) is more peaked and heavier-tailed than the solid normal curve ‚Äî matching real stock data more closely.</figcaption>
  </figure>
  <h3> 9. MARTINGALES</h3>
  <h4>9.1 Definition</h4>
  <p>
    A stochastic process \(X_n : n= 0,1, \ldots \) is said to be martingale if \(&forall; n= 0,1, \ldots\) 
    $$ (a) E[|X_n|] < \infty $$
    $$ (b) E[X_{n+1}| X_0, X_1, \ldots , X_n ] = X_n $$
  </p>
  <p>
    for the (b) part, we observe that $$ (b) E[E[X_{n+1}| X_0, X_1, \ldots , X_n ]] = E[X_n] $$
    also by the tower property, $$ (b) E[E[X_{n+1}| X_0, X_1, \ldots , X_n ]] = E[X_{n+1}] $$
    thus, we can observe that \(E[X_0]= E[X_k] &forall; k =0,1, \ldots \)  
  </p>
  <h4>9.2 Martingales and Fairness in Gambling & Markets</h4>
  <p>
    The martingale property captures the idea of fairness in gambling: a player‚Äôs expected fortune after the next play equals the current fortune, regardless of past outcomes. This reflects a fair game, where no betting strategy (like the classic "martingale" doubling approach) can guarantee profit in the long run. While martingale theory has roots in gambling, it now finds broad applications beyond it.
  </p>
  <p>
    In financial markets, martingales model the notion of efficient pricing. For a security like a stock, if future prices could be predicted, buying or selling pressure would shift the current price. In an ideal, perfect market, such adjustments lead to a situation where future prices are, on average, unpredictable, making the price process a martingale.
  </p>
</section>

<div class="nav-bottom">
  <a href="ch0-preface.html">&larr; Previous Chapter</a>
  <a href="tk-markov-chains1.html">Next Chapter &rarr;</a>
</div>

<footer>
  &copy; 2025 Mohammad Shaan | This site is maintained on GitHub Pages.
</footer>
<!-- MathJax for LaTeX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
<script>
  function toggleHint() {
    var hint = document.getElementById("hint");
    if (hint.style.display === "none" || hint.style.display === "") {
      hint.style.display = "block";
    } else {
      hint.style.display = "none";
    }
  }
</script>

</body>
</html>
