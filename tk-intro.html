<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Stochastic Processes & Finance ‚Äì Summer Reading Project</title>
  <link href="https://fonts.googleapis.com/css2?family=Lato&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Lato', sans-serif;
      margin: 0;
      padding: 0;
      background: #f9f9f9;
      color: #333;
    }
    header {
      background-color: #003366;
      color: white;
      padding: 1em;
      text-align: center;
    }
    nav {
      background-color: #e0e0e0;
      padding: 1em;
      text-align: center;
    }
    nav a {
      margin: 0 1em;
      text-decoration: none;
      color: #003366;
      font-weight: bold;
    }
    section {
      max-width: 1200px;
      margin: 2em auto;
      padding: 1em;
      background: white;
      border-radius: 8px;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }

    .nav-bottom a {
      flex: 1;
      text-align: center;
      padding: 0.5em 1em;
      color: #003366;
      font-weight: bold;
      text-decoration: none;
    }

    footer {
      text-align: center;
      font-size: 0.9em;
      color: #777;
      margin: 2em 0;
    }
  </style>
</head>
<body>

<header>
  <h1>Chapter 1: Introduction to Stochastic Processes</h1>
  <p>Study notes and chapter summaries</p>
</header>

<nav>
  <a href="index.html">Home</a>
  <a href="topics.html">Topics</a>
  <a href="notes.html">Notes</a>
  <a href="projects.html">Programming</a>
</nav>
<section>
  
  <h2>1. What is a Stochastic Process?</h2>
  <p>
    Derived from the Greek, "Stochastic" means "random" or "chance". It's antonym is "sure", "deterministic", or "certain". While a deterministic model ensures a single outcome from a given set of circumstances, a stochastic model predicts the possible set of outcomes with their respective likelihoods or probabilities.
    While a coin tossed in the air will surely land somewhere on the land around us, whether it lands on its heads or tails is random, with each outcome being assigned a probability of 0.5 .
    It's interesting to note that phenomena are not themselves stochastic or deterministic, it is just how the observer perceives the model. For example, changes in the level of a huge population are often considered and modeled deteministically, although we may agree upon the fact that many chance events contribute to their fluctuations. 
  </p>
  <p>
    As for the definition, <strong>A stochastic process is a family of random variables \( X_t \), where t is a parameter running over a suitable index set T.</strong>
    Usually, t indicates the discrete units of time, and the index set becomes  \( T= \{0,1,2, \ldots \} \). Stochastic processes where \( T= [0,\infty) \) become important in different applications
  </p>
  <h2>2. Review of Probability</h2>
  <h3>
    2.1. Random Variables:
  </h3>
  <p>
    Let \( \Omega \) be a non-empty set equipped with a \( \sigma \)-algebra \( \mathcal{F} \). A random variable is a function \( X : \Omega \to \mathbb{R} \) such that for every Borel set \( A \subseteq \mathbb{R} \), we have the pre-image \( X^{-1}(A) \in \mathcal{F} \); i.e., \(X\) is \(\mathcal{F}\) - measurable.
  </p>
  <h3>
    2.2. Continuous and discrete Random Variables:
  </h3>
  <p>
    <li><strong>CDF:</strong> Let \(X\) be a random vaiable. Then its Cumulative Distribution Function (CDF) is defined as \(F : \mathbb{R} \to [0,1] \) given by $$ F(x)= P(X \leq x), \quad x \in \mathbb{R} $$ </li>
    <li><strong>Continuous Distribution:</strong> A random variable is said to be continuous if its CDF is continuous</li>
    <li><strong>Discrete Distributions:</strong> If the CDF of a random variable has all the properties of a CDF but is discontinuous, it is said to be discrete random variable</li>
    <li><strong>Density function:</strong> A function \(f : \mathbb{R} \to [0, \infty) \) is said to be the density function of some rv having CDF \(F\)  iff $$ \int_{-\infty}^{\infty} f(x)\, dx = 1 $$</li>
    <li><strong>Probability Mass Function (PMF):</strong> A discrete random variable \(X\) taking values \( x_1, x_2, \ldots \) with probabilities \( p_1, p_2, \ldots \),respectively. The Probability Mass function (PMF) of \( X \) is given as \( p : \mathbb{R} \to [0, 1] \), where
\[
p(x) =
  \begin{cases}
    p_1 & \text{if } x = x_1 \\
    p_2 & \text{if } x = x_2 \\
    p_3 & \text{if } x = x_3 \\
    . \\
    . \\
    . \\
    0   & \text{otherwise.}
  \end{cases}
\]
where \( \sum p_i = 1 \).
</li>

  <h3>2.3. Moments and Expectations</h3>
  <p>
    If X is a discrete rv, its \(m\)-th moment is given by $$ E[X^m] = \sum_{i} x_i^m \, \Pr(X = x_i) $$
    For continuous rv, the \(m\)-th moment is given by $$ E[X^m] = \int_{-\infty}^{\infty} x^m \, f(x) dx $$
    The \(m\)-th central moment is defined as the \(m\)-th order moment of the random variable \(X- \mu_x \), provided that \(\mu_x\) exists finitely.
    
  </p>
  <h2>3. Some Discrete Distributions</h2>
  <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; margin: 2em auto; text-align: center;">
  <thead style="background-color: #f0f0f0;">
    <tr>
      <th>Distribution</th>
      <th>Notation</th>
      <th>PMF</th>
      <th>Characteristic Function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Bernoulli</td>
      <td>\( X \sim \mathrm{Bern}(p) \)</td>
      <td>\( p(x) = p^x(1-p)^{1-x},\ x \in \{0,1\} \)</td>
      <td>\( \varphi_X(t) = 1 - p + pe^{it} \)</td>
    </tr>
    <tr>
      <td>Binomial</td>
      <td>\( X \sim \mathrm{Bin}(n, p) \)</td>
      <td>\( p(x) = \binom{n}{x}p^x(1-p)^{n-x} \)</td>
      <td>\( \varphi_X(t) = (1 - p + pe^{it})^n \)</td>
    </tr>
    <tr>
      <td>Poisson</td>
      <td>\( X \sim \mathrm{Pois}(\lambda) \)</td>
      <td>\( p(x) = \frac{e^{-\lambda} \lambda^x}{x!} \)</td>
      <td>\( \varphi_X(t) = \exp(\lambda(e^{it} - 1)) \)</td>
    </tr>
    <tr>
      <td>Geometric</td>
      <td>\( X \sim \mathrm{Geom}(n, p) \)</td>
      <td>\( f(x) = p(1-p)^{x-1}, x \in \{1,2, \ldots\} \)</td>
      <td>\( \varphi_X(t) = \frac{pe^{it}}{1 - (1 - p)e^{it}} \)</td>
    </tr>
  </tbody>
  </table>
  
  <h2>4. Some Continuous Distributions</h2>
  <table border="1" cellpadding="8" cellspacing="0" style="border-collapse: collapse; margin: 2em auto; text-align: center;">
  <thead style="background-color: #f0f0f0;">
    <tr>
      <th>Distribution</th>
      <th>Notation</th>
      <th>PDF</th>
      <th>Characteristic Function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Uniform</td>
      <td>\( X \sim \mathrm{Unif}(a,b) \)</td>
      <td>\( f(x) = \frac{1} {b-a},\ x \in [a,b] \)</td>
      <td>\( \varphi_X(t) = \frac{e^{itb}-e^{ita}} {it(b-a)} \)</td>
    </tr>
    <tr>
      <td>Exponential</td>
      <td>\( X \sim \mathrm{Exp}(\lambda) \)</td>
      <td>\( f(x) = \lambda e^{-\lambda x} , \ x \geq 0   \)</td>
      <td>\( \varphi_X(t) = \frac{\lambda}{\lambda -ib} \)</td>
    </tr>
    <tr>
      <td>Gamma</td>
      <td>\( X \sim \mathrm{Gamma}(p (rate), \alpha (shape)) \)</td>
      <td>\( f(x) = \frac{p^{\alpha}x^{\alpha -1}e^{-px}}{\gamma(\alpha)} , \ x > 0 \)</td>
      <td>\( \varphi_X(t) = {(\frac{p}{p-it})}^{\alpha} \)</td>
    </tr>
    <tr>
      <td>Beta</td>
      <td>\( X \sim \mathrm{Beta}(a, b) \)</td>
      <td>\( f(x) = \frac{x^{a-1}(1-x)^{b-1}}{\beta(a,b)}, \ x \in (0,1) \)</td>
      <td>Not elementary in closed form</td>
    </tr>
    <tr>
      <td>Cauchy</td>
      <td>\( X \sim \mathrm{Cauchy}(x_0, \gamma) \)</td>
      <td>\( f(x) = \frac{1}{\pi \gamma \left[1 + \left(\frac{x - x_0}{\gamma}\right)^2 \right]} \)</td>
      <td>\( \varphi_X(t) = e^{i x_0 t - \gamma |t|} \)</td>
    </tr>
    <tr>
      <td>Normal (Univariate)</td>
      <td>\( X \sim \mathcal{N}(\mu, \sigma^2) \)</td>
      <td>\( f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{ -\frac{(x - \mu)^2}{2\sigma^2} } \)</td>
      <td>\( \varphi_X(t) = e^{i \mu t - \frac{1}{2} \sigma^2 t^2} \)</td>
    </tr>
    <tr>
      <td>Normal (Multivariate)</td>
      <td>\( \mathbf{\vec{X}} \sim \mathcal{N}_n(\boldsymbol{\vec{\mu}}, \boldsymbol{\Sigma}) \)</td>
      <td>\( f(\mathbf{\vec{x}}) = \frac{1}{(2\pi)^{n/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{\vec{x}} - \boldsymbol{\vec{\mu}})^\top \boldsymbol{\Sigma}^{-1} (\mathbf{\vec{x}} - \boldsymbol{\vec{\mu}}) \right) \)</td>
      <td>\( \varphi_{\mathbf{X}}(\mathbf{t}) = \exp\left( i \mathbf{t}^\top \boldsymbol{\mu} - \frac{1}{2} \mathbf{t}^\top \boldsymbol{\Sigma} \mathbf{t} \right) \)</td>
    </tr>
    <tr>
      <td>Chi-square</td>
      <td>\( X \sim \chi^2_k \)</td>
      <td>\( f(x) = \frac{1}{2^{k/2} \Gamma(k/2)} x^{k/2 - 1} e^{-x/2},\ x > 0 \)</td>
      <td>\( \varphi_X(t) = (1 - 2it)^{-k/2} \)</td>
    </tr>
    <tr>
      <td>t-distribution</td>
      <td>\( T \sim t_\nu \)</td>
      <td>\( f(t) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu \pi} \ \Gamma\left(\frac{\nu}{2}\right)} \left(1 + \frac{t^2}{\nu}\right)^{-(\nu + 1)/2} \)</td>
      <td>No simple closed form, but the k-th order moment exists only when the degrees of freedom (= n) >k</td>
    </tr>
    <tr>
      <td>F-distribution</td>
      <td>\( F \sim F_{d_1, d_2} \)</td>
      <td>\( f(x) = \frac{1}{B\left(\frac{d_1}{2}, \frac{d_2}{2}\right)} \left( \frac{d_1}{d_2} \right)^{d_1/2} \frac{x^{d_1/2 - 1}}{\left(1 + \frac{d_1}{d_2} x \right)^{(d_1 + d_2)/2}},\ x > 0 \)</td>
      <td>No simple closed form, but the k-th moment exists only if \(d_2\) > 2ùëü </td>
    </tr>
  </tbody>
  </table>
  <h2>5. Inequalities at a glance</h2>
  <h3>5.1 The Schwarz Inequality</h3>
  <p>
    Let X and Y have finite second moments. Then <strong>$$ {E[XY]}^2 \leq E[X^2]E[Y^2] $$</strong>
  </p>
  <h3>5.2 The Markov Inequality</h3>
  <p>
    Let X be a non-negative random variable, and t be a positive number. Then <strong>$$ P(X \geq t) \leq \frac{E[X]}{t} $$</strong>
  </p>
  <h3>5.3 The Chebyshev's Inequality</h3>
  <p>
    Let X be a random variable with mean \( \mu \) and finite variance \( \sigma^2 \) . Then for any real number t > 0  <strong>$$ P(|X- \mu| \geq t) \leq \frac{\sigma^2}{t^2} $$</strong>
  </p>
                                                                                                                                                                             
  <h2>6. Random Sums</h2>
  <p>
    Let \( X_1 , X_2, \ldots \) be a sequence of IID random variables, and let N be a discrete random variable, independent of all \(X_i\)s, and having PMF \( p_N (n) = \Pr (N=n), \ n \in {0,1,2, \ldots } \) We define the random sum \(S_N\) as $$ S_N = \sum_{i=1}^{N} X_i, \ N>0 $$
  </p>
  <ul>
    <li>H. M. Taylor and S. Karlin, *An Introduction to Stochastic Modeling*, Chapter 1</li>
  </ul>
</section>

<div class="nav-bottom">
  <a href="ch0-preface.html">&larr; Previous Chapter</a>
  <a href="tk-markov-chains1.html">Next Chapter &rarr;</a>
</div>

<footer>
  &copy; 2025 Mohammad Shaan | This site is maintained on GitHub Pages.
</footer>
<!-- MathJax for LaTeX -->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

</body>
</html>
